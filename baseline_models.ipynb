{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86966764-fc92-42d8-b084-02ccecb91409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Alireza P. Nouri\n",
    "# Email: apashamoham@miners.utep.edu\n",
    "# this is the code to implement different baseline models to classify pubmed articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5d030cb-b209-4745-9e66-806b6736e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "59b0d142-edc2-4985-a66c-70c5ca2b9051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04045b63-bec2-4ed8-8411-c615391d4951",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\", ResourceWarning)\n",
    "warnings.filterwarnings('always') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8ed4dccd-257b-4554-a42f-393bf822a41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34973/34973 [52:18<00:00, 11.14it/s] \n"
     ]
    }
   ],
   "source": [
    "# generate bert embedding:\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset_reader import dataset_loader\n",
    "from contextual_embedding.contextual_embedding import bert_embedding\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "contextual_bert_dict= dict()\n",
    "context_em = bert_embedding()\n",
    "\n",
    "for id in tqdm(abstract_id_dict.keys()):\n",
    "    temp_text = str(title_id_dict[id])+str(' ') +str(abstract_id_dict[id])\n",
    "    contextual_bert_dict[id] =  context_em.embedding_generator(temp_text, embedding_type='summing')[1][0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8cdd9e16-d35f-4ad3-9887-a7f80ac9f03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1012541/1708544064.py:1: ResourceWarning: unclosed file <_io.BufferedWriter name='01_contextual_bert_dict.p'>\n",
      "  pickle.dump(contextual_bert_dict, open(\"01_contextual_bert_dict.p\", \"wb\"))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(contextual_bert_dict, open(\"01_contextual_bert_dict.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70a818c6-e222-48a6-a8b3-9c15e7c988d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1012541/2909961077.py:1: ResourceWarning: unclosed file <_io.BufferedReader name='01_stories.p'>\n",
      "  stories = pickle.load(open('01_stories.p','rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/tmp/ipykernel_1012541/2909961077.py:2: ResourceWarning: unclosed file <_io.BufferedReader name='01_title_id_dict.p'>\n",
      "  title_id_dict = pickle.load(open('01_title_id_dict.p', 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/tmp/ipykernel_1012541/2909961077.py:3: ResourceWarning: unclosed file <_io.BufferedReader name='01_abstract_id_dict.p'>\n",
      "  abstract_id_dict = pickle.load(open('01_abstract_id_dict.p', 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/tmp/ipykernel_1012541/2909961077.py:4: ResourceWarning: unclosed file <_io.BufferedReader name='01_text_id_dict.p'>\n",
      "  text_id_dict = pickle.load(open('01_text_id_dict.p', 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/tmp/ipykernel_1012541/2909961077.py:5: ResourceWarning: unclosed file <_io.BufferedReader name='01_date_id_dict.p'>\n",
      "  date_id_dict = pickle.load(open('01_date_id_dict.p', 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/tmp/ipykernel_1012541/2909961077.py:6: ResourceWarning: unclosed file <_io.BufferedReader name='01_meshMajor_id_dict.p'>\n",
      "  meshMajor_id_dict = pickle.load(open('01_meshMajor_id_dict.p', 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/tmp/ipykernel_1012541/2909961077.py:7: ResourceWarning: unclosed file <_io.BufferedReader name='01_pmid_id_dict.p'>\n",
      "  pmid_id_dict = pickle.load(open('01_pmid_id_dict.p', 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/tmp/ipykernel_1012541/2909961077.py:8: ResourceWarning: unclosed file <_io.BufferedReader name='01_meshid_id_dict.p'>\n",
      "  meshid_id_dict = pickle.load(open('01_meshid_id_dict.p', 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/tmp/ipykernel_1012541/2909961077.py:9: ResourceWarning: unclosed file <_io.BufferedReader name='01_meshroot_id_dict.p'>\n",
      "  meshroot_id_dict = pickle.load(open('01_meshroot_id_dict.p', 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/tmp/ipykernel_1012541/2909961077.py:10: ResourceWarning: unclosed file <_io.BufferedReader name='01_train_id.p'>\n",
      "  train_id = pickle.load(open('01_train_id.p', 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/tmp/ipykernel_1012541/2909961077.py:11: ResourceWarning: unclosed file <_io.BufferedReader name='01_tf_idf_dict.p'>\n",
      "  tf_idf_dict = pickle.load(open('01_tf_idf_dict.p', 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/tmp/ipykernel_1012541/2909961077.py:12: ResourceWarning: unclosed file <_io.BufferedReader name='01_contextual_doc_embedding.p'>\n",
      "  contextual_doc_embedding = pickle.load(open('01_contextual_doc_embedding.p', 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/tmp/ipykernel_1012541/2909961077.py:13: ResourceWarning: unclosed file <_io.BufferedReader name='01_contextual_bert_embedding.p'>\n",
      "  contextual_bert_embedding = pickle.load(open('01_contextual_bert_embedding.p', 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/tmp/ipykernel_1012541/2909961077.py:14: ResourceWarning: unclosed file <_io.BufferedReader name='01_contextual_dict.p'>\n",
      "  contextual_dict = pickle.load(open('01_contextual_dict.p','rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/tmp/ipykernel_1012541/2909961077.py:15: ResourceWarning: unclosed file <_io.BufferedReader name='01_contextual_doc_embedding.p'>\n",
      "  contextual_doc_embedding = pickle.load(open('01_contextual_doc_embedding.p', 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/tmp/ipykernel_1012541/2909961077.py:16: ResourceWarning: unclosed file <_io.BufferedReader name='01_dataset.p'>\n",
      "  dataset = pickle.load(open('01_dataset.p', 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/tmp/ipykernel_1012541/2909961077.py:17: ResourceWarning: unclosed file <_io.BufferedReader name='01_final_stories.p'>\n",
      "  new_stories = pickle.load(open('01_final_stories.p', 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stories = pickle.load(open('01_stories.p','rb'))\n",
    "title_id_dict = pickle.load(open('01_title_id_dict.p', 'rb'))\n",
    "abstract_id_dict = pickle.load(open('01_abstract_id_dict.p', 'rb'))\n",
    "text_id_dict = pickle.load(open('01_text_id_dict.p', 'rb'))\n",
    "date_id_dict = pickle.load(open('01_date_id_dict.p', 'rb'))\n",
    "meshMajor_id_dict = pickle.load(open('01_meshMajor_id_dict.p', 'rb'))\n",
    "pmid_id_dict = pickle.load(open('01_pmid_id_dict.p', 'rb'))\n",
    "meshid_id_dict = pickle.load(open('01_meshid_id_dict.p', 'rb'))\n",
    "meshroot_id_dict = pickle.load(open('01_meshroot_id_dict.p', 'rb'))\n",
    "train_id = pickle.load(open('01_train_id.p', 'rb'))\n",
    "tf_idf_dict = pickle.load(open('01_tf_idf_dict.p', 'rb'))\n",
    "contextual_doc_embedding = pickle.load(open('01_contextual_doc_embedding.p', 'rb'))\n",
    "contextual_bert_embedding = pickle.load(open('01_contextual_bert_embedding.p', 'rb'))\n",
    "contextual_dict = pickle.load(open('01_contextual_dict.p','rb'))\n",
    "contextual_doc_embedding = pickle.load(open('01_contextual_doc_embedding.p', 'rb'))\n",
    "dataset = pickle.load(open('01_dataset.p', 'rb'))\n",
    "new_stories = pickle.load(open('01_final_stories.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "cde292de-9b73-4ec0-9bb7-5e6af09e7355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1012541/331937566.py:1: ResourceWarning: unclosed file <_io.BufferedReader name='01_final_stories.p'>\n",
      "  new_stories_1 = pickle.load(open('01_final_stories.p', 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "new_stories_1 = pickle.load(open('02_final_stories.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73d93bd6-3390-4053-980d-036af1356600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_id: all ids in stories generated by difstorygen\n",
    "# total_id: all ids in dataset that has timestamp\n",
    "# test_id: all ids in dataset that has timestamp but they are not in stories generated by difstorygen\n",
    "# total_dataset: a list of data in dataset with values of id, title, abstract, and list of meshterms \n",
    "# total_train: all data in doc_id with values of id, title, abstract, and list of meshterms\n",
    "# total_test: all data in test_id with values of id, title, abstract, and list of meshterms\n",
    "# stories_neighbors: a dictionary with all neighbors of documents generated by difstorygen\n",
    "# training_ids: a list of ids of data with neighbors generated by difstorygen \n",
    "# testing_ids: a list of ids of data from total_id that not in training_ids\n",
    "\n",
    "\n",
    "# tf_idf_neighbors: dictionary of tf-idf of enighbors of document in training_ids\n",
    "# contextual_neighbors: a dictionary of contextual embedding of documents in training_ids\n",
    "\n",
    "\n",
    "# print(\"doc_id length is {}\".format(len(doc_id))) \n",
    "# print(\"total_id length is {}\".format(len(total_id))) \n",
    "# print(\"test_id length is {}\".format(len(test_id)))\n",
    "# print(\"total_dataset length is {}\".format(len(total_dataset))) \n",
    "# print(\"total_train length is {}\".format(len(total_train))) \n",
    "# print(\"total_test length is {}\".format(len(total_test)))\n",
    "# print(\"stories_neighbors length is {}\".format(len(stories_neighbors))) \n",
    "# print(\"training_ids length is {}\".format(len(training_ids))) \n",
    "# print(\"testing_ids length is {}\".format(len(testing_ids))) \n",
    "# print(\"tf_idf_neighbors length is {}\".format(len(tf_idf_neighbors))) \n",
    "# print(\"contextual_neighbors length is {}\".format(len(contextual_neighbors))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c8a5d75-0fa1-404c-8c70-0b040d2ee8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of difStoryGen_stories dict is 300\n"
     ]
    }
   ],
   "source": [
    "# new_stories = pickle.load(open('01_final_stories.p', 'rb'))\n",
    "\n",
    "difStoryGen_stories = dict()\n",
    "for i in new_stories.keys():\n",
    "    difStoryGen_stories[i] = new_stories[i]\n",
    "for i in stories.keys():\n",
    "    difStoryGen_stories[i] = stories[i]\n",
    "print('Size of difStoryGen_stories dict is {}'.format(len(difStoryGen_stories)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "5567c7a5-bd87-49ef-b65c-ba5bcf1b5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in new_stories_1.keys():\n",
    "    difStoryGen_stories[i] = new_stories_1[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "ffd0dc45-6c09-4302-9ee8-900e445885a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of difStoryGen_stories dict is 300\n"
     ]
    }
   ],
   "source": [
    "print('Size of difStoryGen_stories dict is {}'.format(len(difStoryGen_stories)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c903b6fa-49c1-4a3b-9d8f-815385374783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lable size is 14\n"
     ]
    }
   ],
   "source": [
    "label_dict= dict()\n",
    "for d in dataset:\n",
    "    label_dict[d['id']] = list(d.values())[7:-1]\n",
    "label_size = len(label_dict[list(label_dict.keys())[0]]) \n",
    "print('lable size is {}'.format(label_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dcc2cda-07b6-40f4-b98b-637cd2504f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of difstoryGen_ids ids is 2572\n"
     ]
    }
   ],
   "source": [
    "difstoryGen_ids = set()\n",
    "for i in difStoryGen_stories.keys():\n",
    "    for j in difStoryGen_stories[i]:\n",
    "        difstoryGen_ids.add(j)\n",
    "print('Size of difstoryGen_ids ids is {}'.format(len(difstoryGen_ids)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28db5125-683f-4d97-b487-1e18e2237cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of total_dataset_ids ids is 34973\n"
     ]
    }
   ],
   "source": [
    "total_dataset_ids = set()\n",
    "for i in title_id_dict:\n",
    "    total_dataset_ids.add(i)\n",
    "print('Size of total_dataset_ids ids is {}'.format(len(total_dataset_ids)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98ac19fd-bd05-43de-b503-da4ea754106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of not_difstoryGen_ids ids is 32401\n"
     ]
    }
   ],
   "source": [
    "not_difstoryGen_ids = set()\n",
    "for i in total_dataset_ids:\n",
    "    if i not in difstoryGen_ids:\n",
    "        not_difstoryGen_ids.add(i)\n",
    "print('Size of not_difstoryGen_ids ids is {}'.format(len(not_difstoryGen_ids)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7655be3-02cc-466a-b5c7-15584fe210c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training dataset for convertor is 643\n"
     ]
    }
   ],
   "source": [
    "n_sampling_for_converter = int(len(difstoryGen_ids)/4)\n",
    "testing_for_converter_ids = random.sample(list(not_difstoryGen_ids), n_sampling_for_converter)\n",
    "print('Size of training dataset for convertor is {}'.format(len(testing_for_converter_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c339533-2c46-4460-b615-a6eaba545573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of contextual embedding ids is 27544\n"
     ]
    }
   ],
   "source": [
    "contextual_ids = set()\n",
    "for i in contextual_dict.keys():\n",
    "    contextual_ids.add(i)\n",
    "print('Size of contextual embedding ids is {}'.format(len(contextual_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db781423-59ed-455c-ac0f-291eb470696a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of tf_idf_ids ids is 34973\n"
     ]
    }
   ],
   "source": [
    "tf_idf_ids = set()\n",
    "for i in tf_idf_dict.keys():\n",
    "    tf_idf_ids.add(i)\n",
    "print('Size of tf_idf_ids ids is {}'.format(len(tf_idf_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f224884-3c2b-4309-95e9-fdd02626aad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of stories_neighbors ids is 2572\n"
     ]
    }
   ],
   "source": [
    "#generate the neighbors\n",
    "stories_neighbors = dict()\n",
    "for i in difStoryGen_stories:\n",
    "    for j in difStoryGen_stories[i]:\n",
    "        stories_neighbors[j] = []\n",
    "\n",
    "for i in list(difStoryGen_stories.keys())[1:-1]:\n",
    "    if len(difStoryGen_stories[i]) < 2:\n",
    "        continue\n",
    "    stories_neighbors[difStoryGen_stories[i][0]].append(difStoryGen_stories[i][1])\n",
    "    for j in range(1,len(difStoryGen_stories[i])-1):\n",
    "        stories_neighbors[difStoryGen_stories[i][j]].append(difStoryGen_stories[i][j+1])\n",
    "        stories_neighbors[difStoryGen_stories[i][j]].append(difStoryGen_stories[i][j-1])\n",
    "    stories_neighbors[difStoryGen_stories[i][-1]].append(difStoryGen_stories[i][-2])\n",
    "print('Size of stories_neighbors ids is {}'.format(len(stories_neighbors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da934e2a-c185-4b00-81ee-f0a59ddb7d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f48c43e-186f-4ec0-9d38-4ef071b043d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6a6aab7a-92c9-42b0-971e-fd5c83516916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2544/2544 [00:00<00:00, 63253.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of contextual_neighbors dict is 2544\n",
      "Size of training_for_convertor_ids list is 2544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2544/2544 [00:00<00:00, 57452.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of contextual_neighbors dict is 2544\n",
      "Size of training_for_convertor_ids list is 2544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_for_convertor_ids  = [i for i in stories_neighbors.keys() if len(stories_neighbors[i])>0]\n",
    "# #generate tf-idf of neighbors:\n",
    "# tf_idf_neighbors = dict()\n",
    "# for i in tqdm(training_ids):\n",
    "#     tf_idf_neighbors[i] = np.average([tf_idf_dict[j] for j in stories_neighbors[i]], axis=0)\n",
    "\n",
    "#generate contextual embedding of neighbors\n",
    "contextual_neighbors = dict()\n",
    "for i in tqdm(training_for_convertor_ids):\n",
    "    contextual_neighbors[i] = np.average([contextual_dict[j] for j in stories_neighbors[i]], axis=0)\n",
    "print('Size of contextual_neighbors dict is {}'.format(len(contextual_neighbors)))\n",
    "print('Size of training_for_convertor_ids list is {}'.format(len(training_for_convertor_ids)))\n",
    "\n",
    "\n",
    "contextual_neighbors_768 = dict()\n",
    "for i in tqdm(training_for_convertor_ids):\n",
    "    contextual_neighbors_768[i] = np.average([contextual_bert_dict[j] for j in stories_neighbors[i]], axis=0)\n",
    "print('Size of contextual_neighbors dict is {}'.format(len(contextual_neighbors_768)))\n",
    "print('Size of training_for_convertor_ids list is {}'.format(len(training_for_convertor_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afc82fcb-4688-4d5b-a37a-f68fbfe408bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25664 (100.25 KB)\n",
      "Trainable params: 25280 (98.75 KB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4868.5366 - accuracy: 0.0440\n",
      "Epoch 2/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4738.8755 - accuracy: 0.0377\n",
      "Epoch 3/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4715.5615 - accuracy: 0.0409\n",
      "Epoch 4/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4722.3149 - accuracy: 0.0480\n",
      "Epoch 5/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4735.6826 - accuracy: 0.0417\n",
      "Epoch 6/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4711.8418 - accuracy: 0.0393\n",
      "Epoch 7/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4697.8853 - accuracy: 0.0358\n",
      "Epoch 8/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4684.4727 - accuracy: 0.0440\n",
      "Epoch 9/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4674.5459 - accuracy: 0.0476\n",
      "Epoch 10/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4704.4233 - accuracy: 0.0480\n",
      "Epoch 11/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4715.2236 - accuracy: 0.0428\n",
      "Epoch 12/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4689.0029 - accuracy: 0.0389\n",
      "Epoch 13/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4682.9346 - accuracy: 0.0417\n",
      "Epoch 14/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4683.3003 - accuracy: 0.0342\n",
      "Epoch 15/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4696.3867 - accuracy: 0.0326\n",
      "Epoch 16/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4729.1099 - accuracy: 0.0385\n",
      "Epoch 17/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4682.1665 - accuracy: 0.0267\n",
      "Epoch 18/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4704.8252 - accuracy: 0.0275\n",
      "Epoch 19/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4677.5288 - accuracy: 0.0216\n",
      "Epoch 20/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4682.2183 - accuracy: 0.0295\n",
      "Epoch 21/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4694.4009 - accuracy: 0.0299\n",
      "Epoch 22/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4688.8647 - accuracy: 0.0263\n",
      "Epoch 23/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4672.9883 - accuracy: 0.0244\n",
      "Epoch 24/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4685.8091 - accuracy: 0.0228\n",
      "Epoch 25/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4699.2510 - accuracy: 0.0228\n",
      "Epoch 26/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4681.5493 - accuracy: 0.0256\n",
      "Epoch 27/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4669.0879 - accuracy: 0.0236\n",
      "Epoch 28/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4670.3564 - accuracy: 0.0177\n",
      "Epoch 29/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4675.7524 - accuracy: 0.0181\n",
      "Epoch 30/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4660.2485 - accuracy: 0.0153\n",
      "Epoch 31/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4683.2114 - accuracy: 0.0193\n",
      "Epoch 32/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4679.2397 - accuracy: 0.0149\n",
      "Epoch 33/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4660.6543 - accuracy: 0.0177\n",
      "Epoch 34/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4716.1641 - accuracy: 0.0177\n",
      "Epoch 35/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4665.0049 - accuracy: 0.0153\n",
      "Epoch 36/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4683.3921 - accuracy: 0.0153\n",
      "Epoch 37/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4723.4106 - accuracy: 0.0114\n",
      "Epoch 38/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4668.8940 - accuracy: 0.0138\n",
      "Epoch 39/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4673.2729 - accuracy: 0.0090\n",
      "Epoch 40/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4659.4443 - accuracy: 0.0067\n",
      "Epoch 41/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4689.2314 - accuracy: 0.0102\n",
      "Epoch 42/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4668.3735 - accuracy: 0.0102\n",
      "Epoch 43/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4684.9727 - accuracy: 0.0118\n",
      "Epoch 44/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4670.9717 - accuracy: 0.0090\n",
      "Epoch 45/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4679.6260 - accuracy: 0.0098\n",
      "Epoch 46/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4680.4941 - accuracy: 0.0110\n",
      "Epoch 47/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4696.0229 - accuracy: 0.0122\n",
      "Epoch 48/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4668.5234 - accuracy: 0.0083\n",
      "Epoch 49/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4701.8682 - accuracy: 0.0083\n",
      "Epoch 50/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4684.3779 - accuracy: 0.0086\n",
      "Epoch 51/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4687.7505 - accuracy: 0.0098\n",
      "Epoch 52/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4681.7104 - accuracy: 0.0098\n",
      "Epoch 53/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4719.4785 - accuracy: 0.0122\n",
      "Epoch 54/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4670.4795 - accuracy: 0.0122\n",
      "Epoch 55/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4677.5171 - accuracy: 0.0114\n",
      "Epoch 56/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4689.6772 - accuracy: 0.0106\n",
      "Epoch 57/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4685.6846 - accuracy: 0.0106\n",
      "Epoch 58/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4668.5811 - accuracy: 0.0086\n",
      "Epoch 59/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4678.8721 - accuracy: 0.0094\n",
      "Epoch 60/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4690.1582 - accuracy: 0.0094\n",
      "Epoch 61/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4668.1895 - accuracy: 0.0094\n",
      "Epoch 62/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4699.7300 - accuracy: 0.0075\n",
      "Epoch 63/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4676.8208 - accuracy: 0.0063\n",
      "Epoch 64/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4679.6084 - accuracy: 0.0090\n",
      "Epoch 65/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4706.2793 - accuracy: 0.0094\n",
      "Epoch 66/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4673.7642 - accuracy: 0.0098\n",
      "Epoch 67/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4705.4932 - accuracy: 0.0071\n",
      "Epoch 68/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4669.4111 - accuracy: 0.0067\n",
      "Epoch 69/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4661.6465 - accuracy: 0.0102\n",
      "Epoch 70/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4679.1948 - accuracy: 0.0106\n",
      "Epoch 71/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4658.9355 - accuracy: 0.0126\n",
      "Epoch 72/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4691.7817 - accuracy: 0.0142\n",
      "Epoch 73/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4704.2324 - accuracy: 0.0130\n",
      "Epoch 74/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4685.4849 - accuracy: 0.0126\n",
      "Epoch 75/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4680.2217 - accuracy: 0.0145\n",
      "Epoch 76/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4687.3994 - accuracy: 0.0142\n",
      "Epoch 77/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4690.2451 - accuracy: 0.0153\n",
      "Epoch 78/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4674.6699 - accuracy: 0.0145\n",
      "Epoch 79/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4787.9019 - accuracy: 0.0138\n",
      "Epoch 80/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4717.5864 - accuracy: 0.0169\n",
      "Epoch 81/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4673.9751 - accuracy: 0.0173\n",
      "Epoch 82/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4680.4888 - accuracy: 0.0200\n",
      "Epoch 83/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4672.4321 - accuracy: 0.0208\n",
      "Epoch 84/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4659.4590 - accuracy: 0.0240\n",
      "Epoch 85/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4702.7129 - accuracy: 0.0228\n",
      "Epoch 86/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4679.3682 - accuracy: 0.0224\n",
      "Epoch 87/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4707.4067 - accuracy: 0.0236\n",
      "Epoch 88/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4680.4165 - accuracy: 0.0228\n",
      "Epoch 89/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4684.8179 - accuracy: 0.0291\n",
      "Epoch 90/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4668.8135 - accuracy: 0.0220\n",
      "Epoch 91/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4695.3501 - accuracy: 0.0263\n",
      "Epoch 92/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4662.7261 - accuracy: 0.0208\n",
      "Epoch 93/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4702.4380 - accuracy: 0.0216\n",
      "Epoch 94/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4711.6538 - accuracy: 0.0267\n",
      "Epoch 95/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4663.7358 - accuracy: 0.0236\n",
      "Epoch 96/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4686.0171 - accuracy: 0.0224\n",
      "Epoch 97/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4706.2109 - accuracy: 0.0256\n",
      "Epoch 98/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4701.3306 - accuracy: 0.0236\n",
      "Epoch 99/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4687.3843 - accuracy: 0.0263\n",
      "Epoch 100/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4685.7959 - accuracy: 0.0267\n",
      "Epoch 101/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4683.0957 - accuracy: 0.0224\n",
      "Epoch 102/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4712.6704 - accuracy: 0.0244\n",
      "Epoch 103/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4679.6533 - accuracy: 0.0240\n",
      "Epoch 104/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4685.8770 - accuracy: 0.0208\n",
      "Epoch 105/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4695.7983 - accuracy: 0.0189\n",
      "Epoch 106/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4685.1143 - accuracy: 0.0173\n",
      "Epoch 107/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4686.0015 - accuracy: 0.0212\n",
      "Epoch 108/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4676.9966 - accuracy: 0.0177\n",
      "Epoch 109/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4679.4800 - accuracy: 0.0204\n",
      "Epoch 110/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4670.2246 - accuracy: 0.0193\n",
      "Epoch 111/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4728.0532 - accuracy: 0.0252\n",
      "Epoch 112/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4698.0156 - accuracy: 0.0181\n",
      "Epoch 113/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4685.2368 - accuracy: 0.0220\n",
      "Epoch 114/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4697.4893 - accuracy: 0.0248\n",
      "Epoch 115/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4699.1865 - accuracy: 0.0232\n",
      "Epoch 116/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4704.5332 - accuracy: 0.0248\n",
      "Epoch 117/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4683.9023 - accuracy: 0.0275\n",
      "Epoch 118/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4683.8125 - accuracy: 0.0256\n",
      "Epoch 119/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4666.6265 - accuracy: 0.0248\n",
      "Epoch 120/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4672.7368 - accuracy: 0.0275\n",
      "Epoch 121/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4699.0312 - accuracy: 0.0283\n",
      "Epoch 122/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4702.9062 - accuracy: 0.0248\n",
      "Epoch 123/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4724.9136 - accuracy: 0.0256\n",
      "Epoch 124/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4720.2856 - accuracy: 0.0263\n",
      "Epoch 125/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4732.1006 - accuracy: 0.0291\n",
      "Epoch 126/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4727.7119 - accuracy: 0.0271\n",
      "Epoch 127/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4717.9746 - accuracy: 0.0295\n",
      "Epoch 128/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4683.6807 - accuracy: 0.0232\n",
      "Epoch 129/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4656.3267 - accuracy: 0.0248\n",
      "Epoch 130/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4669.0112 - accuracy: 0.0256\n",
      "Epoch 131/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4658.1431 - accuracy: 0.0204\n",
      "Epoch 132/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4692.9712 - accuracy: 0.0240\n",
      "Epoch 133/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4701.3965 - accuracy: 0.0169\n",
      "Epoch 134/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4671.5083 - accuracy: 0.0185\n",
      "Epoch 135/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4690.3013 - accuracy: 0.0181\n",
      "Epoch 136/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4683.0132 - accuracy: 0.0208\n",
      "Epoch 137/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4681.3008 - accuracy: 0.0216\n",
      "Epoch 138/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4677.8394 - accuracy: 0.0236\n",
      "Epoch 139/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4698.3926 - accuracy: 0.0185\n",
      "Epoch 140/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4680.0029 - accuracy: 0.0216\n",
      "Epoch 141/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4702.8223 - accuracy: 0.0244\n",
      "Epoch 142/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4685.4487 - accuracy: 0.0244\n",
      "Epoch 143/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4681.9238 - accuracy: 0.0267\n",
      "Epoch 144/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4682.9941 - accuracy: 0.0267\n",
      "Epoch 145/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4690.2305 - accuracy: 0.0173\n",
      "Epoch 146/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4676.3032 - accuracy: 0.0185\n",
      "Epoch 147/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4687.8940 - accuracy: 0.0126\n",
      "Epoch 148/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4665.1704 - accuracy: 0.0126\n",
      "Epoch 149/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4691.1719 - accuracy: 0.0165\n",
      "Epoch 150/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4669.1250 - accuracy: 0.0169\n",
      "Epoch 151/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4666.6260 - accuracy: 0.0165\n",
      "Epoch 152/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4678.4976 - accuracy: 0.0149\n",
      "Epoch 153/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4673.4448 - accuracy: 0.0193\n",
      "Epoch 154/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4676.5010 - accuracy: 0.0165\n",
      "Epoch 155/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4677.9331 - accuracy: 0.0153\n",
      "Epoch 156/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4661.1968 - accuracy: 0.0181\n",
      "Epoch 157/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4671.4453 - accuracy: 0.0181\n",
      "Epoch 158/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4684.2935 - accuracy: 0.0204\n",
      "Epoch 159/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4689.4136 - accuracy: 0.0177\n",
      "Epoch 160/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4664.5752 - accuracy: 0.0224\n",
      "Epoch 161/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4692.9951 - accuracy: 0.0165\n",
      "Epoch 162/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4657.8877 - accuracy: 0.0204\n",
      "Epoch 163/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4669.1914 - accuracy: 0.0216\n",
      "Epoch 164/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4661.7759 - accuracy: 0.0240\n",
      "Epoch 165/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4659.3398 - accuracy: 0.0216\n",
      "Epoch 166/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4708.4253 - accuracy: 0.0204\n",
      "Epoch 167/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4673.6035 - accuracy: 0.0181\n",
      "Epoch 168/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4664.3701 - accuracy: 0.0149\n",
      "Epoch 169/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4656.7915 - accuracy: 0.0165\n",
      "Epoch 170/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4659.7803 - accuracy: 0.0138\n",
      "Epoch 171/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4655.0693 - accuracy: 0.0134\n",
      "Epoch 172/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4665.0059 - accuracy: 0.0142\n",
      "Epoch 173/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4670.7812 - accuracy: 0.0118\n",
      "Epoch 174/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4667.9219 - accuracy: 0.0138\n",
      "Epoch 175/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4655.7832 - accuracy: 0.0130\n",
      "Epoch 176/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4666.3433 - accuracy: 0.0142\n",
      "Epoch 177/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4691.8521 - accuracy: 0.0185\n",
      "Epoch 178/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4670.4526 - accuracy: 0.0193\n",
      "Epoch 179/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4687.3628 - accuracy: 0.0228\n",
      "Epoch 180/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4645.5977 - accuracy: 0.0193\n",
      "Epoch 181/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4675.6528 - accuracy: 0.0153\n",
      "Epoch 182/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4688.5635 - accuracy: 0.0161\n",
      "Epoch 183/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4692.1299 - accuracy: 0.0149\n",
      "Epoch 184/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4662.3892 - accuracy: 0.0142\n",
      "Epoch 185/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4659.9771 - accuracy: 0.0259\n",
      "Epoch 186/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4657.7368 - accuracy: 0.0275\n",
      "Epoch 187/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4668.8232 - accuracy: 0.0259\n",
      "Epoch 188/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4647.1787 - accuracy: 0.0248\n",
      "Epoch 189/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4648.4126 - accuracy: 0.0256\n",
      "Epoch 190/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4656.4644 - accuracy: 0.0216\n",
      "Epoch 191/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4658.7100 - accuracy: 0.0224\n",
      "Epoch 192/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4654.4800 - accuracy: 0.0256\n",
      "Epoch 193/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4686.5244 - accuracy: 0.0220\n",
      "Epoch 194/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4658.9414 - accuracy: 0.0240\n",
      "Epoch 195/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4667.0928 - accuracy: 0.0220\n",
      "Epoch 196/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4656.0195 - accuracy: 0.0185\n",
      "Epoch 197/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4664.8584 - accuracy: 0.0248\n",
      "Epoch 198/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4672.6187 - accuracy: 0.0208\n",
      "Epoch 199/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4669.2305 - accuracy: 0.0193\n",
      "Epoch 200/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4727.5298 - accuracy: 0.0204\n",
      "Epoch 201/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4674.7539 - accuracy: 0.0157\n",
      "Epoch 202/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4653.1318 - accuracy: 0.0185\n",
      "Epoch 203/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4656.3604 - accuracy: 0.0224\n",
      "Epoch 204/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4702.9438 - accuracy: 0.0165\n",
      "Epoch 205/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4676.8979 - accuracy: 0.0173\n",
      "Epoch 206/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4671.1543 - accuracy: 0.0165\n",
      "Epoch 207/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4701.6167 - accuracy: 0.0189\n",
      "Epoch 208/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4663.4087 - accuracy: 0.0189\n",
      "Epoch 209/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4697.1279 - accuracy: 0.0177\n",
      "Epoch 210/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4675.4507 - accuracy: 0.0232\n",
      "Epoch 211/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4660.4854 - accuracy: 0.0232\n",
      "Epoch 212/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4682.5322 - accuracy: 0.0220\n",
      "Epoch 213/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4694.1279 - accuracy: 0.0244\n",
      "Epoch 214/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4688.2080 - accuracy: 0.0200\n",
      "Epoch 215/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4676.1968 - accuracy: 0.0220\n",
      "Epoch 216/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4698.8496 - accuracy: 0.0169\n",
      "Epoch 217/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4684.1084 - accuracy: 0.0200\n",
      "Epoch 218/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4671.0835 - accuracy: 0.0228\n",
      "Epoch 219/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4656.3774 - accuracy: 0.0271\n",
      "Epoch 220/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4667.5312 - accuracy: 0.0256\n",
      "Epoch 221/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4670.0176 - accuracy: 0.0240\n",
      "Epoch 222/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4677.7871 - accuracy: 0.0208\n",
      "Epoch 223/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4656.7241 - accuracy: 0.0232\n",
      "Epoch 224/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4682.5693 - accuracy: 0.0228\n",
      "Epoch 225/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4684.3042 - accuracy: 0.0200\n",
      "Epoch 226/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4654.6470 - accuracy: 0.0204\n",
      "Epoch 227/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4663.0034 - accuracy: 0.0169\n",
      "Epoch 228/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4668.4263 - accuracy: 0.0197\n",
      "Epoch 229/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4661.1191 - accuracy: 0.0189\n",
      "Epoch 230/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4651.3018 - accuracy: 0.0165\n",
      "Epoch 231/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4664.1885 - accuracy: 0.0193\n",
      "Epoch 232/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4680.7886 - accuracy: 0.0181\n",
      "Epoch 233/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4681.1006 - accuracy: 0.0212\n",
      "Epoch 234/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4695.6460 - accuracy: 0.0208\n",
      "Epoch 235/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4697.1255 - accuracy: 0.0248\n",
      "Epoch 236/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4647.0479 - accuracy: 0.0271\n",
      "Epoch 237/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4643.7544 - accuracy: 0.0204\n",
      "Epoch 238/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4666.5635 - accuracy: 0.0224\n",
      "Epoch 239/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4679.4175 - accuracy: 0.0248\n",
      "Epoch 240/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4677.4033 - accuracy: 0.0271\n",
      "Epoch 241/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4673.8960 - accuracy: 0.0244\n",
      "Epoch 242/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4677.7236 - accuracy: 0.0212\n",
      "Epoch 243/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4691.3369 - accuracy: 0.0228\n",
      "Epoch 244/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4670.8262 - accuracy: 0.0224\n",
      "Epoch 245/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4654.2197 - accuracy: 0.0208\n",
      "Epoch 246/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4654.7832 - accuracy: 0.0256\n",
      "Epoch 247/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4657.0137 - accuracy: 0.0216\n",
      "Epoch 248/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4683.0264 - accuracy: 0.0232\n",
      "Epoch 249/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4657.0708 - accuracy: 0.0216\n",
      "Epoch 250/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4682.1128 - accuracy: 0.0189\n",
      "Epoch 251/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4695.5391 - accuracy: 0.0220\n",
      "Epoch 252/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4653.8872 - accuracy: 0.0252\n",
      "Epoch 253/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4640.8721 - accuracy: 0.0220\n",
      "Epoch 254/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4670.9614 - accuracy: 0.0216\n",
      "Epoch 255/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4664.8203 - accuracy: 0.0189\n",
      "Epoch 256/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4688.2432 - accuracy: 0.0216\n",
      "Epoch 257/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4664.5776 - accuracy: 0.0204\n",
      "Epoch 258/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4660.7524 - accuracy: 0.0173\n",
      "Epoch 259/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4670.0474 - accuracy: 0.0197\n",
      "Epoch 260/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4673.7363 - accuracy: 0.0193\n",
      "Epoch 261/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4661.3452 - accuracy: 0.0189\n",
      "Epoch 262/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4674.3193 - accuracy: 0.0165\n",
      "Epoch 263/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4692.2993 - accuracy: 0.0197\n",
      "Epoch 264/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4636.6890 - accuracy: 0.0177\n",
      "Epoch 265/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4662.2549 - accuracy: 0.0200\n",
      "Epoch 266/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4659.1763 - accuracy: 0.0181\n",
      "Epoch 267/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4678.9521 - accuracy: 0.0185\n",
      "Epoch 268/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4648.8906 - accuracy: 0.0185\n",
      "Epoch 269/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4652.7925 - accuracy: 0.0165\n",
      "Epoch 270/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4679.9204 - accuracy: 0.0193\n",
      "Epoch 271/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4654.4443 - accuracy: 0.0208\n",
      "Epoch 272/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4691.7588 - accuracy: 0.0193\n",
      "Epoch 273/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4671.6313 - accuracy: 0.0181\n",
      "Epoch 274/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4672.4082 - accuracy: 0.0189\n",
      "Epoch 275/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4674.2349 - accuracy: 0.0216\n",
      "Epoch 276/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4656.6738 - accuracy: 0.0212\n",
      "Epoch 277/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4675.6177 - accuracy: 0.0204\n",
      "Epoch 278/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4662.8457 - accuracy: 0.0259\n",
      "Epoch 279/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4683.5977 - accuracy: 0.0228\n",
      "Epoch 280/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4656.3701 - accuracy: 0.0232\n",
      "Epoch 281/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4637.7588 - accuracy: 0.0299\n",
      "Epoch 282/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4664.6426 - accuracy: 0.0256\n",
      "Epoch 283/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4693.1582 - accuracy: 0.0248\n",
      "Epoch 284/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4659.6582 - accuracy: 0.0212\n",
      "Epoch 285/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4673.1621 - accuracy: 0.0232\n",
      "Epoch 286/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 4673.6001 - accuracy: 0.0212\n",
      "Epoch 287/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4652.3560 - accuracy: 0.0200\n",
      "Epoch 288/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4668.0801 - accuracy: 0.0208\n",
      "Epoch 289/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4650.4878 - accuracy: 0.0193\n",
      "Epoch 290/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4671.0420 - accuracy: 0.0193\n",
      "Epoch 291/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4665.9277 - accuracy: 0.0220\n",
      "Epoch 292/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4665.1167 - accuracy: 0.0216\n",
      "Epoch 293/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4670.5024 - accuracy: 0.0200\n",
      "Epoch 294/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4661.6953 - accuracy: 0.0181\n",
      "Epoch 295/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4663.0132 - accuracy: 0.0200\n",
      "Epoch 296/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 4676.1284 - accuracy: 0.0193\n",
      "Epoch 297/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4673.0532 - accuracy: 0.0193\n",
      "Epoch 298/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4654.6948 - accuracy: 0.0224\n",
      "Epoch 299/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4664.9507 - accuracy: 0.0228\n",
      "Epoch 300/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 4643.1499 - accuracy: 0.0208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f7d4814ed10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a neural network to predict the neighbors for any given contextual embedding\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "X = np.array([contextual_dict[i] for i in training_for_convertor_ids]) #training data for the neural network to generate the neighbors embedding vector\n",
    "y = np.array([contextual_neighbors[i] for i in training_for_convertor_ids]) #training data for the neural network to map the neighbors embedding vector\n",
    "# Define the model\n",
    "def mlp_model(input_dim, output_dim):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,)),  # First hidden layer with 64 neurons and ReLU activation\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "        Dense(128, activation='relu'),  # Second hidden layer with 32 neurons and ReLU activation\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "        Dense(64, activation='relu'), \n",
    "        Dense(output_dim, activation='linear')  # Output layer with 'output_dim' neurons\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_dim = X.shape[1] # Size of the input vector (n)\n",
    "output_dim = y.shape[1]  # Size of the output vector (m)\n",
    "\n",
    "# Create the MLP model\n",
    "contextual_model = mlp_model(input_dim, output_dim)\n",
    "\n",
    "# Model summary to see the architecture\n",
    "contextual_model.summary()\n",
    "\n",
    "\n",
    "# Train the model\n",
    "contextual_model.fit(X, y, epochs=300, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4073c3ca-6788-4a76-a7ae-92ea56c58078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 1s 624us/step\n"
     ]
    }
   ],
   "source": [
    "Z = np.array([contextual_dict[i] for i in not_difstoryGen_ids if i in contextual_ids])\n",
    "\n",
    "generated_neighbors = contextual_model.predict(Z)\n",
    "\n",
    "Z_ids = [i for i in not_difstoryGen_ids if i in contextual_ids]\n",
    "\n",
    "generated_contexual_dict = dict()\n",
    "for i,j in zip(Z_ids, generated_neighbors):\n",
    "    generated_contexual_dict[i] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "774b0807-1578-4eda-bc41-c5cd2fb70b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 64)                49216     \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 768)               49920     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 116480 (455.00 KB)\n",
      "Trainable params: 116096 (453.50 KB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "159/159 [==============================] - 1s 2ms/step - loss: -313.0747 - accuracy: 0.0067\n",
      "Epoch 2/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -3393.6243 - accuracy: 0.0000e+00\n",
      "Epoch 3/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 1360.3683 - accuracy: 0.0000e+00\n",
      "Epoch 4/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -3363.2559 - accuracy: 0.0000e+00\n",
      "Epoch 5/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 415.9613 - accuracy: 0.0000e+00\n",
      "Epoch 6/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1787.5225 - accuracy: 0.0000e+00\n",
      "Epoch 7/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2760.9128 - accuracy: 0.0000e+00\n",
      "Epoch 8/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 233.0855 - accuracy: 7.8616e-04\n",
      "Epoch 9/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -945.8716 - accuracy: 7.8616e-04\n",
      "Epoch 10/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2371.5208 - accuracy: 0.0016\n",
      "Epoch 11/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1424.1439 - accuracy: 3.9308e-04\n",
      "Epoch 12/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -865.2056 - accuracy: 3.9308e-04\n",
      "Epoch 13/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2173.6863 - accuracy: 0.0028\n",
      "Epoch 14/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1438.9032 - accuracy: 0.0012\n",
      "Epoch 15/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 11.5994 - accuracy: 0.0016\n",
      "Epoch 16/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -957.4250 - accuracy: 0.0012\n",
      "Epoch 17/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2095.3594 - accuracy: 0.0024\n",
      "Epoch 18/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2680.6521 - accuracy: 7.8616e-04\n",
      "Epoch 19/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1889.3934 - accuracy: 3.9308e-04\n",
      "Epoch 20/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 214.6521 - accuracy: 0.0016\n",
      "Epoch 21/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1014.2047 - accuracy: 3.9308e-04\n",
      "Epoch 22/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2334.6934 - accuracy: 7.8616e-04\n",
      "Epoch 23/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1206.0485 - accuracy: 0.0012\n",
      "Epoch 24/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -2551.3291 - accuracy: 0.0000e+00\n",
      "Epoch 25/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 48.0005 - accuracy: 0.0016\n",
      "Epoch 26/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -487.3839 - accuracy: 0.0024\n",
      "Epoch 27/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -404.4667 - accuracy: 0.0016\n",
      "Epoch 28/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1662.5405 - accuracy: 0.0020\n",
      "Epoch 29/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2512.7837 - accuracy: 0.0028\n",
      "Epoch 30/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1256.4823 - accuracy: 0.0020\n",
      "Epoch 31/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -742.3644 - accuracy: 0.0016\n",
      "Epoch 32/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -515.5022 - accuracy: 0.0012\n",
      "Epoch 33/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -601.2028 - accuracy: 7.8616e-04\n",
      "Epoch 34/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2332.2686 - accuracy: 0.0020\n",
      "Epoch 35/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1379.0172 - accuracy: 0.0031\n",
      "Epoch 36/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1388.7535 - accuracy: 0.0031\n",
      "Epoch 37/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -434.2970 - accuracy: 0.0031\n",
      "Epoch 38/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -649.6771 - accuracy: 0.0016\n",
      "Epoch 39/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2007.6602 - accuracy: 0.0028\n",
      "Epoch 40/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1383.4858 - accuracy: 0.0035\n",
      "Epoch 41/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2034.6215 - accuracy: 0.0012\n",
      "Epoch 42/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2317.2363 - accuracy: 3.9308e-04\n",
      "Epoch 43/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -247.1581 - accuracy: 0.0012\n",
      "Epoch 44/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1635.6340 - accuracy: 0.0035\n",
      "Epoch 45/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1312.6134 - accuracy: 7.8616e-04\n",
      "Epoch 46/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1217.1570 - accuracy: 0.0012\n",
      "Epoch 47/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1878.5869 - accuracy: 0.0016\n",
      "Epoch 48/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -593.9793 - accuracy: 0.0031\n",
      "Epoch 49/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2547.4299 - accuracy: 0.0020\n",
      "Epoch 50/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 56.8251 - accuracy: 0.0012\n",
      "Epoch 51/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2448.4553 - accuracy: 0.0016\n",
      "Epoch 52/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -589.9225 - accuracy: 0.0028\n",
      "Epoch 53/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1010.8428 - accuracy: 0.0020\n",
      "Epoch 54/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -364.6850 - accuracy: 0.0016\n",
      "Epoch 55/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2167.8967 - accuracy: 0.0020\n",
      "Epoch 56/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -730.7686 - accuracy: 0.0024\n",
      "Epoch 57/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2181.4465 - accuracy: 0.0012\n",
      "Epoch 58/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -3527.9995 - accuracy: 0.0031\n",
      "Epoch 59/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 301.2449 - accuracy: 0.0024\n",
      "Epoch 60/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2087.7515 - accuracy: 0.0020\n",
      "Epoch 61/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -3116.9956 - accuracy: 0.0020\n",
      "Epoch 62/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 1394.2515 - accuracy: 0.0012\n",
      "Epoch 63/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1947.4781 - accuracy: 7.8616e-04\n",
      "Epoch 64/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1148.1812 - accuracy: 7.8616e-04\n",
      "Epoch 65/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2137.6873 - accuracy: 0.0020\n",
      "Epoch 66/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1823.2437 - accuracy: 0.0028\n",
      "Epoch 67/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -716.6440 - accuracy: 0.0016\n",
      "Epoch 68/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2672.9634 - accuracy: 0.0012\n",
      "Epoch 69/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -123.4456 - accuracy: 0.0028\n",
      "Epoch 70/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -3220.0872 - accuracy: 0.0012\n",
      "Epoch 71/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 50.2066 - accuracy: 0.0028\n",
      "Epoch 72/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1992.5637 - accuracy: 0.0020\n",
      "Epoch 73/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1584.3793 - accuracy: 0.0012\n",
      "Epoch 74/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2309.1729 - accuracy: 0.0024\n",
      "Epoch 75/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -205.9000 - accuracy: 7.8616e-04\n",
      "Epoch 76/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -862.5751 - accuracy: 0.0016\n",
      "Epoch 77/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -771.7549 - accuracy: 0.0024\n",
      "Epoch 78/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2634.0776 - accuracy: 0.0035\n",
      "Epoch 79/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -135.2674 - accuracy: 0.0028\n",
      "Epoch 80/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2574.3159 - accuracy: 0.0016\n",
      "Epoch 81/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 65.0919 - accuracy: 0.0016\n",
      "Epoch 82/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2284.9663 - accuracy: 0.0024\n",
      "Epoch 83/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -855.1939 - accuracy: 0.0024\n",
      "Epoch 84/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1502.0604 - accuracy: 0.0016\n",
      "Epoch 85/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1027.6941 - accuracy: 0.0024\n",
      "Epoch 86/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1592.7319 - accuracy: 0.0012\n",
      "Epoch 87/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -715.9449 - accuracy: 0.0020\n",
      "Epoch 88/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1775.8799 - accuracy: 0.0024\n",
      "Epoch 89/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -768.4813 - accuracy: 0.0020\n",
      "Epoch 90/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2593.4016 - accuracy: 0.0012\n",
      "Epoch 91/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -266.7368 - accuracy: 0.0016\n",
      "Epoch 92/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2163.6609 - accuracy: 0.0031\n",
      "Epoch 93/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1143.6764 - accuracy: 0.0031\n",
      "Epoch 94/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1006.8712 - accuracy: 0.0016\n",
      "Epoch 95/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2260.9077 - accuracy: 0.0016\n",
      "Epoch 96/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -705.7678 - accuracy: 0.0016\n",
      "Epoch 97/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -3157.2019 - accuracy: 0.0031\n",
      "Epoch 98/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -393.7977 - accuracy: 0.0035\n",
      "Epoch 99/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1397.4996 - accuracy: 0.0024\n",
      "Epoch 100/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2579.2993 - accuracy: 0.0016\n",
      "Epoch 101/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -151.6498 - accuracy: 0.0016\n",
      "Epoch 102/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2903.2725 - accuracy: 0.0012\n",
      "Epoch 103/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 169.4062 - accuracy: 0.0012\n",
      "Epoch 104/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2453.0530 - accuracy: 0.0024\n",
      "Epoch 105/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -208.9191 - accuracy: 0.0028\n",
      "Epoch 106/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2753.8904 - accuracy: 0.0016\n",
      "Epoch 107/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -73.5476 - accuracy: 0.0024\n",
      "Epoch 108/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2493.3140 - accuracy: 0.0016\n",
      "Epoch 109/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1245.2108 - accuracy: 0.0024\n",
      "Epoch 110/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1336.9813 - accuracy: 0.0020\n",
      "Epoch 111/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2106.4917 - accuracy: 0.0028\n",
      "Epoch 112/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -724.3427 - accuracy: 0.0012\n",
      "Epoch 113/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2731.2976 - accuracy: 0.0016\n",
      "Epoch 114/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -131.4881 - accuracy: 0.0035\n",
      "Epoch 115/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2575.5657 - accuracy: 0.0012\n",
      "Epoch 116/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -340.4274 - accuracy: 0.0024\n",
      "Epoch 117/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2943.4583 - accuracy: 0.0028\n",
      "Epoch 118/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -213.1659 - accuracy: 0.0016\n",
      "Epoch 119/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2753.8508 - accuracy: 0.0020\n",
      "Epoch 120/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -426.8072 - accuracy: 0.0024\n",
      "Epoch 121/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2775.2529 - accuracy: 0.0024\n",
      "Epoch 122/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -177.0754 - accuracy: 0.0016\n",
      "Epoch 123/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2318.6208 - accuracy: 0.0024\n",
      "Epoch 124/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1081.2937 - accuracy: 0.0031\n",
      "Epoch 125/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1280.2136 - accuracy: 0.0024\n",
      "Epoch 126/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2839.9360 - accuracy: 0.0012\n",
      "Epoch 127/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -284.1367 - accuracy: 7.8616e-04\n",
      "Epoch 128/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -3037.0098 - accuracy: 0.0024\n",
      "Epoch 129/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -134.3916 - accuracy: 0.0028\n",
      "Epoch 130/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2629.7595 - accuracy: 0.0024\n",
      "Epoch 131/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -454.1322 - accuracy: 0.0012\n",
      "Epoch 132/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2449.8049 - accuracy: 0.0020\n",
      "Epoch 133/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -421.0718 - accuracy: 0.0028\n",
      "Epoch 134/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2406.7349 - accuracy: 0.0031\n",
      "Epoch 135/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -801.3336 - accuracy: 0.0028\n",
      "Epoch 136/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1420.0203 - accuracy: 0.0028\n",
      "Epoch 137/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1917.1388 - accuracy: 0.0016\n",
      "Epoch 138/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1278.0667 - accuracy: 0.0024\n",
      "Epoch 139/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1687.2976 - accuracy: 0.0028\n",
      "Epoch 140/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2161.1873 - accuracy: 0.0024\n",
      "Epoch 141/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1088.6367 - accuracy: 7.8616e-04\n",
      "Epoch 142/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1809.1091 - accuracy: 0.0024\n",
      "Epoch 143/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1081.7039 - accuracy: 0.0028\n",
      "Epoch 144/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2637.4402 - accuracy: 0.0024\n",
      "Epoch 145/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -465.9337 - accuracy: 0.0035\n",
      "Epoch 146/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2978.6628 - accuracy: 0.0047\n",
      "Epoch 147/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -12.1439 - accuracy: 0.0028\n",
      "Epoch 148/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2750.0647 - accuracy: 0.0016\n",
      "Epoch 149/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -414.3893 - accuracy: 0.0039\n",
      "Epoch 150/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2666.9209 - accuracy: 0.0035\n",
      "Epoch 151/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -624.3529 - accuracy: 0.0031\n",
      "Epoch 152/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1788.0767 - accuracy: 0.0020\n",
      "Epoch 153/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1612.1643 - accuracy: 0.0028\n",
      "Epoch 154/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1497.1591 - accuracy: 0.0028\n",
      "Epoch 155/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2025.7008 - accuracy: 0.0024\n",
      "Epoch 156/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -817.9393 - accuracy: 0.0028\n",
      "Epoch 157/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2687.6470 - accuracy: 0.0031\n",
      "Epoch 158/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -553.3325 - accuracy: 0.0047\n",
      "Epoch 159/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2330.1440 - accuracy: 0.0043\n",
      "Epoch 160/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -901.9581 - accuracy: 0.0047\n",
      "Epoch 161/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -1774.9756 - accuracy: 0.0031\n",
      "Epoch 162/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -1403.4686 - accuracy: 0.0051\n",
      "Epoch 163/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -1772.6460 - accuracy: 0.0031\n",
      "Epoch 164/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -1349.0077 - accuracy: 0.0059\n",
      "Epoch 165/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2352.8701 - accuracy: 0.0047\n",
      "Epoch 166/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -742.4147 - accuracy: 0.0035\n",
      "Epoch 167/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2254.6606 - accuracy: 0.0051\n",
      "Epoch 168/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -1194.2656 - accuracy: 0.0055\n",
      "Epoch 169/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -3152.4771 - accuracy: 0.0051\n",
      "Epoch 170/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -202.3774 - accuracy: 0.0043\n",
      "Epoch 171/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2685.7712 - accuracy: 0.0035\n",
      "Epoch 172/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -970.8533 - accuracy: 0.0059\n",
      "Epoch 173/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -2169.5513 - accuracy: 0.0055\n",
      "Epoch 174/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1145.5128 - accuracy: 0.0047\n",
      "Epoch 175/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1930.5295 - accuracy: 0.0031\n",
      "Epoch 176/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1707.1691 - accuracy: 0.0055\n",
      "Epoch 177/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1688.0684 - accuracy: 0.0051\n",
      "Epoch 178/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1655.8560 - accuracy: 0.0055\n",
      "Epoch 179/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1643.6041 - accuracy: 0.0059\n",
      "Epoch 180/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1816.1530 - accuracy: 0.0047\n",
      "Epoch 181/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1426.3969 - accuracy: 0.0051\n",
      "Epoch 182/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1650.1600 - accuracy: 0.0039\n",
      "Epoch 183/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1712.5554 - accuracy: 0.0059\n",
      "Epoch 184/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1344.2809 - accuracy: 0.0043\n",
      "Epoch 185/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2061.0823 - accuracy: 0.0039\n",
      "Epoch 186/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1016.9564 - accuracy: 0.0051\n",
      "Epoch 187/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -2553.9832 - accuracy: 0.0028\n",
      "Epoch 188/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -732.6108 - accuracy: 0.0063\n",
      "Epoch 189/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2969.3301 - accuracy: 0.0047\n",
      "Epoch 190/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -472.8551 - accuracy: 0.0043\n",
      "Epoch 191/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2593.5959 - accuracy: 0.0059\n",
      "Epoch 192/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -846.1134 - accuracy: 0.0051\n",
      "Epoch 193/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2445.3708 - accuracy: 0.0055\n",
      "Epoch 194/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1240.0023 - accuracy: 0.0055\n",
      "Epoch 195/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1801.6025 - accuracy: 0.0047\n",
      "Epoch 196/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1738.9906 - accuracy: 0.0051\n",
      "Epoch 197/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1457.9982 - accuracy: 0.0043\n",
      "Epoch 198/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1823.0197 - accuracy: 0.0035\n",
      "Epoch 199/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1585.6564 - accuracy: 0.0035\n",
      "Epoch 200/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2084.4910 - accuracy: 0.0047\n",
      "Epoch 201/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1188.3674 - accuracy: 0.0055\n",
      "Epoch 202/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2213.1296 - accuracy: 0.0063\n",
      "Epoch 203/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1041.7170 - accuracy: 0.0043\n",
      "Epoch 204/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2502.9846 - accuracy: 0.0067\n",
      "Epoch 205/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -810.9768 - accuracy: 0.0043\n",
      "Epoch 206/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2884.9006 - accuracy: 0.0055\n",
      "Epoch 207/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -588.2441 - accuracy: 0.0063\n",
      "Epoch 208/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2495.9834 - accuracy: 0.0055\n",
      "Epoch 209/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -858.4588 - accuracy: 0.0055\n",
      "Epoch 210/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2184.8711 - accuracy: 0.0067\n",
      "Epoch 211/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1414.1262 - accuracy: 0.0055\n",
      "Epoch 212/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1460.5890 - accuracy: 0.0055\n",
      "Epoch 213/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2565.4204 - accuracy: 0.0055\n",
      "Epoch 214/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -676.4590 - accuracy: 0.0047\n",
      "Epoch 215/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2792.7681 - accuracy: 0.0063\n",
      "Epoch 216/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -687.9260 - accuracy: 0.0059\n",
      "Epoch 217/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2092.0015 - accuracy: 0.0059\n",
      "Epoch 218/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1185.3551 - accuracy: 0.0043\n",
      "Epoch 219/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2761.5100 - accuracy: 0.0063\n",
      "Epoch 220/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -802.1960 - accuracy: 0.0055\n",
      "Epoch 221/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1834.6034 - accuracy: 0.0055\n",
      "Epoch 222/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1756.0078 - accuracy: 0.0051\n",
      "Epoch 223/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2334.9995 - accuracy: 0.0055\n",
      "Epoch 224/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -964.9541 - accuracy: 0.0051\n",
      "Epoch 225/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2344.2346 - accuracy: 0.0055\n",
      "Epoch 226/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1205.2865 - accuracy: 0.0051\n",
      "Epoch 227/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2083.3030 - accuracy: 0.0059\n",
      "Epoch 228/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1657.2036 - accuracy: 0.0059\n",
      "Epoch 229/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1373.8164 - accuracy: 0.0051\n",
      "Epoch 230/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -2511.9580 - accuracy: 0.0063\n",
      "Epoch 231/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -778.7346 - accuracy: 0.0059\n",
      "Epoch 232/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2828.7942 - accuracy: 0.0055\n",
      "Epoch 233/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -614.9982 - accuracy: 0.0055\n",
      "Epoch 234/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2674.2817 - accuracy: 0.0059\n",
      "Epoch 235/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -778.4834 - accuracy: 0.0059\n",
      "Epoch 236/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2160.0110 - accuracy: 0.0059\n",
      "Epoch 237/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1351.1642 - accuracy: 0.0059\n",
      "Epoch 238/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2475.3452 - accuracy: 0.0059\n",
      "Epoch 239/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1017.0055 - accuracy: 0.0059\n",
      "Epoch 240/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2917.6365 - accuracy: 0.0063\n",
      "Epoch 241/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -565.5314 - accuracy: 0.0055\n",
      "Epoch 242/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -3112.5388 - accuracy: 0.0059\n",
      "Epoch 243/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -546.6586 - accuracy: 0.0059\n",
      "Epoch 244/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2688.2571 - accuracy: 0.0055\n",
      "Epoch 245/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -929.8881 - accuracy: 0.0059\n",
      "Epoch 246/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2638.9895 - accuracy: 0.0059\n",
      "Epoch 247/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -942.4865 - accuracy: 0.0059\n",
      "Epoch 248/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2108.2693 - accuracy: 0.0055\n",
      "Epoch 249/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1737.0804 - accuracy: 0.0059\n",
      "Epoch 250/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1526.0895 - accuracy: 0.0059\n",
      "Epoch 251/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1722.8351 - accuracy: 0.0059\n",
      "Epoch 252/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1710.5980 - accuracy: 0.0055\n",
      "Epoch 253/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -1532.6144 - accuracy: 0.0059\n",
      "Epoch 254/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2246.0540 - accuracy: 0.0059\n",
      "Epoch 255/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -792.5882 - accuracy: 0.0059\n",
      "Epoch 256/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2654.0510 - accuracy: 0.0051\n",
      "Epoch 257/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -766.5269 - accuracy: 0.0059\n",
      "Epoch 258/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -2548.6255 - accuracy: 0.0059\n",
      "Epoch 259/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -855.9710 - accuracy: 0.0059\n",
      "Epoch 260/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -2445.9570 - accuracy: 0.0059\n",
      "Epoch 261/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -878.5159 - accuracy: 0.0063\n",
      "Epoch 262/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2496.8469 - accuracy: 0.0059\n",
      "Epoch 263/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1078.5673 - accuracy: 0.0059\n",
      "Epoch 264/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1842.6266 - accuracy: 0.0059\n",
      "Epoch 265/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1476.9370 - accuracy: 0.0055\n",
      "Epoch 266/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2247.3716 - accuracy: 0.0059\n",
      "Epoch 267/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1210.5004 - accuracy: 0.0059\n",
      "Epoch 268/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -2270.9924 - accuracy: 0.0055\n",
      "Epoch 269/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: -1187.0553 - accuracy: 0.0059\n",
      "Epoch 270/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2320.0366 - accuracy: 0.0059\n",
      "Epoch 271/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1247.1777 - accuracy: 0.0059\n",
      "Epoch 272/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1909.6344 - accuracy: 0.0059\n",
      "Epoch 273/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1978.6404 - accuracy: 0.0059\n",
      "Epoch 274/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -1127.9025 - accuracy: 0.0059\n",
      "Epoch 275/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2652.5601 - accuracy: 0.0059\n",
      "Epoch 276/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -983.9647 - accuracy: 0.0059\n",
      "Epoch 277/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2827.4963 - accuracy: 0.0059\n",
      "Epoch 278/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -745.2336 - accuracy: 0.0059\n",
      "Epoch 279/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2699.4963 - accuracy: 0.0059\n",
      "Epoch 280/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -897.8929 - accuracy: 0.0059\n",
      "Epoch 281/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2479.9077 - accuracy: 0.0059\n",
      "Epoch 282/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1071.8047 - accuracy: 0.0059\n",
      "Epoch 283/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -3021.6287 - accuracy: 0.0059\n",
      "Epoch 284/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -643.8716 - accuracy: 0.0059\n",
      "Epoch 285/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2635.6790 - accuracy: 0.0059\n",
      "Epoch 286/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1139.7455 - accuracy: 0.0059\n",
      "Epoch 287/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -2651.7466 - accuracy: 0.0059\n",
      "Epoch 288/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -1155.7408 - accuracy: 0.0059\n",
      "Epoch 289/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2001.2230 - accuracy: 0.0059\n",
      "Epoch 290/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -1780.3734 - accuracy: 0.0059\n",
      "Epoch 291/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -1499.4696 - accuracy: 0.0059\n",
      "Epoch 292/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2449.9775 - accuracy: 0.0059\n",
      "Epoch 293/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -899.4968 - accuracy: 0.0055\n",
      "Epoch 294/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -2472.5110 - accuracy: 0.0059\n",
      "Epoch 295/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1185.7289 - accuracy: 0.0059\n",
      "Epoch 296/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2332.8462 - accuracy: 0.0059\n",
      "Epoch 297/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: -1178.9343 - accuracy: 0.0059\n",
      "Epoch 298/300\n",
      "159/159 [==============================] - 1s 3ms/step - loss: -2409.2673 - accuracy: 0.0059\n",
      "Epoch 299/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -1009.9304 - accuracy: 0.0059\n",
      "Epoch 300/300\n",
      "159/159 [==============================] - 1s 4ms/step - loss: -2859.1064 - accuracy: 0.0059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f7c0c66f6a0>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a neural network to predict the neighbors for any given contextual embedding\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "X = np.array([contextual_neighbors_768[i] for i in training_for_convertor_ids]) #training data for the neural network to generate the neighbors embedding vector\n",
    "y = np.array([contextual_neighbors_768[i] for i in training_for_convertor_ids]) #training data for the neural network to map the neighbors embedding vector\n",
    "# Define the model\n",
    "def mlp_model(input_dim, output_dim):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,)),  # First hidden layer with 64 neurons and ReLU activation\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "        Dense(128, activation='relu'),  # Second hidden layer with 32 neurons and ReLU activation\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "        Dense(64, activation='relu'), \n",
    "        Dense(output_dim, activation='linear')  # Output layer with 'output_dim' neurons\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_dim = X.shape[1] # Size of the input vector (n)\n",
    "output_dim = y.shape[1]  # Size of the output vector (m)\n",
    "\n",
    "# Create the MLP model\n",
    "contextual_model_768 = mlp_model(input_dim, output_dim)\n",
    "\n",
    "# Model summary to see the architecture\n",
    "contextual_model_768.summary()\n",
    "\n",
    "\n",
    "# Train the model\n",
    "contextual_model_768.fit(X, y, epochs=300, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ca4ab901-a98a-4e3a-98a7-c3c1e1631529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 690us/step\n"
     ]
    }
   ],
   "source": [
    "Z_768 = np.array([contextual_neighbors_768[i] for i in contextual_neighbors_768.keys()])\n",
    "\n",
    "generated_neighbors_768 = contextual_model_768.predict(Z_768)\n",
    "\n",
    "Z_ids_768 = [i for i in not_difstoryGen_ids if i in contextual_bert_dict.keys()]\n",
    "\n",
    "generated_contexual_dict_768 = dict()\n",
    "for i,j in zip(Z_ids, generated_neighbors_768):\n",
    "    generated_contexual_dict_768[i] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dc8fcb-a651-42d9-94b6-0b74681b2f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317b5b1d-7727-49e6-ba58-8437b2664fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5abf64bb-a956-4a38-9273-7fce7161ac54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of generated_contexual_dict dict is 24972\n"
     ]
    }
   ],
   "source": [
    "print('Size of generated_contexual_dict dict is {}'.format(len(generated_contexual_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b89f40c4-39b3-4274-81cc-022e761054b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_total = len(generated_contexual_dict)\n",
    "ids_for_training_basedlines = list(generated_contexual_dict.keys())[int(l_total/5):]\n",
    "ids_for_test_basedlines = list(generated_contexual_dict.keys())[:int(l_total/5)]\n",
    "print('training size is {}'.format(len(ids_for_training_basedlines)))\n",
    "print('test size is {}'.format(len(ids_for_test_basedlines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8dfecf35-7d58-44b7-be32-f976c4346010",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig = np.array([contextual_dict[i] for i in ids_for_training_basedlines])\n",
    "X_test_orig = np.array([contextual_dict[i] for i in ids_for_test_basedlines])\n",
    "X_train_orig_768 = np.array([contextual_bert_dict[i] for i in ids_for_training_basedlines])\n",
    "X_test_orig_768 = np.array([contextual_bert_dict[i] for i in ids_for_test_basedlines])\n",
    "X_train_generated = np.array([np.array(list(generated_contexual_dict[i])+list(contextual_dict[i])) for i in ids_for_training_basedlines])\n",
    "X_test_generated = np.array([np.array(list(generated_contexual_dict[i])+ list(contextual_dict[i])) for i in ids_for_test_basedlines])\n",
    "X_train_generated_768 = np.array([np.array(list(generated_contexual_dict_768[i])+list(contextual_bert_dict[i])) for i in list(generated_contexual_dict_768.keys())[:2000]])\n",
    "X_test_generated_768 = np.array([np.array(list(generated_contexual_dict_768[i])+ list(contextual_bert_dict[i])) for i in list(generated_contexual_dict_768.keys())[2000:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "346fdacf-7a38-4e78-a0e9-c589ef2303b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2544"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_contexual_dict_768.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b3b75acb-1830-4df1-a7eb-80232923cc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 26.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for naive bayes, using contextual embedding vectors without difstoryGen\n",
      "Average  of accuracy:  0.7637164597517022\n",
      "Average of percision:  0.7059762154437905\n",
      "Average of recall:  0.7637164597517022\n",
      "Average of f1 score:  0.6886155262473939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on MultinomialNB\n",
    "accuracy_reslts_MultinomialNB_c = []\n",
    "precision_results_MultinomialNB_c = []\n",
    "recall_results_MultinomialNB_c = []\n",
    "f1_results_MultinomialNB_c = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in ids_for_training_basedlines]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in ids_for_test_basedlines]  # list of categories based on MeSH terms\n",
    "    \n",
    "    p_orig = Pipeline([('Normalizing',MinMaxScaler()),('MultinomialNB',MultinomialNB())])\n",
    "    # model = MultinomialNB()\n",
    "    p_orig.fit(X_train_orig, y_train)\n",
    "    \n",
    "    y_pred = p_orig.predict(X_test_orig)\n",
    "    accuracy_reslts_MultinomialNB_c.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_MultinomialNB_c.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_MultinomialNB_c.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_MultinomialNB_c.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for naive bayes, using contextual embedding vectors without difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_MultinomialNB_c))\n",
    "print(\"Average of percision: \", np.average(precision_results_MultinomialNB_c))\n",
    "print(\"Average of recall: \", np.average(recall_results_MultinomialNB_c))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_MultinomialNB_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8da49fd5-e769-4275-976e-a118cc1128ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 26.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for naive bayes, using contextual embedding 64 vectors with difstoryGen\n",
      "Average  of accuracy:  0.7663195835002004\n",
      "Average of percision:  0.7054111290455791\n",
      "Average of recall:  0.7663195835002004\n",
      "Average of f1 score:  0.6942685660820348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on MultinomialNB\n",
    "\n",
    "accuracy_reslts_MultinomialNB_g = []\n",
    "precision_results_MultinomialNB_g = []\n",
    "recall_results_MultinomialNB_g = []\n",
    "f1_results_MultinomialNB_g = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in ids_for_training_basedlines]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in ids_for_test_basedlines]  # list of categories based on MeSH terms\n",
    "    p = Pipeline([('Normalizing',MinMaxScaler()),('MultinomialNB',MultinomialNB())])\n",
    "    # model = MultinomialNB()\n",
    "    \n",
    "    p.fit(X_train_generated , y_train)\n",
    "    \n",
    "    y_pred = p.predict(X_test_generated)\n",
    "    accuracy_reslts_MultinomialNB_g.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_MultinomialNB_g.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_MultinomialNB_g.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_MultinomialNB_g.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for naive bayes, using contextual embedding 64 vectors with difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_MultinomialNB_g))\n",
    "print(\"Average of percision: \", np.average(precision_results_MultinomialNB_g))\n",
    "print(\"Average of recall: \", np.average(recall_results_MultinomialNB_g))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_MultinomialNB_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "12490bca-82ff-4b9a-95f5-1f4cb716fdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 52.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for naive bayes, using bert 768 vectors with difstoryGen\n",
      "Average  of accuracy:  0.8048844537815124\n",
      "Average of percision:  0.7716651512509944\n",
      "Average of recall:  0.8048844537815124\n",
      "Average of f1 score:  0.7808605133866585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on MultinomialNB\n",
    "accuracy_reslts_MultinomialNB_c768 = []\n",
    "precision_results_MultinomialNB_c768 = []\n",
    "recall_results_MultinomialNB_c768 = []\n",
    "f1_results_MultinomialNB_c768 = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in list(generated_contexual_dict_768.keys())[:2000]]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in list(generated_contexual_dict_768.keys())[2000:]]  # list of categories based on MeSH terms\n",
    "    \n",
    "    p_orig = Pipeline([('Normalizing',MinMaxScaler()),('MultinomialNB',MultinomialNB())])\n",
    "    # model = MultinomialNB()\n",
    "    p_orig.fit(X_train_generated_768, y_train)\n",
    "    \n",
    "    y_pred = p_orig.predict(X_test_generated_768)\n",
    "    accuracy_reslts_MultinomialNB_c768.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_MultinomialNB_c768.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_MultinomialNB_c768.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_MultinomialNB_c768.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for naive bayes, using bert 768 vectors with difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_MultinomialNB_c768))\n",
    "print(\"Average of percision: \", np.average(precision_results_MultinomialNB_c768))\n",
    "print(\"Average of recall: \", np.average(recall_results_MultinomialNB_c768))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_MultinomialNB_c768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "b521235d-6f22-44be-a5cc-57bff6393e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:38<00:00,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for svm, using contextual bert embedding vectors without difstoryGen\n",
      "Average  of accuracy:  0.8394215916242349\n",
      "Average of percision:  0.8330968351287306\n",
      "Average of recall:  0.8394215916242349\n",
      "Average of f1 score:  0.8157445198055508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#SVM classification without difstorygen based on contextual bert document embedding\n",
    "\n",
    "accuracy_reslts_svm_c = []\n",
    "precision_results_svm_c = []\n",
    "recall_results_svm_c = []\n",
    "f1_results_svm_c = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in ids_for_training_basedlines]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in ids_for_test_basedlines] # list of categories based on MeSH terms\n",
    "    \n",
    "    model_svm_c = svm.SVC()\n",
    "    model_svm_c.fit(X_train_orig, y_train)\n",
    "    \n",
    "    y_pred = model_svm_c.predict(X_test_orig)\n",
    "    accuracy_reslts_svm_c.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_svm_c.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_svm_c.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_svm_c.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "import numpy as np\n",
    "print(\"results for svm, using contextual bert embedding vectors without difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_svm_c))\n",
    "print(\"Average of percision: \", np.average(precision_results_svm_c))\n",
    "print(\"Average of recall: \", np.average(recall_results_svm_c))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_svm_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "eb80a9e0-f76f-4082-a693-c15aea1c7b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:09<00:00,  9.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for svm, using contextual embedding 64 vectors with difstoryGen\n",
      "Average  of accuracy:  0.8323988786543853\n",
      "Average of percision:  0.787869736613021\n",
      "Average of recall:  0.8323988786543853\n",
      "Average of f1 score:  0.8040846439358439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#SVM classification with difstorygen based on contextual bert (64) document embedding\n",
    "\n",
    "accuracy_reslts_svm_g = []\n",
    "precision_results_svm_g = []\n",
    "recall_results_svm_g = []\n",
    "f1_results_svm_g = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in ids_for_training_basedlines]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in ids_for_test_basedlines]  # list of categories based on MeSH terms\n",
    "    p_svm = Pipeline([('Normalizing',MinMaxScaler()),('SVM',svm.SVC())])\n",
    "    \n",
    "    \n",
    "    p_svm.fit(X_train_generated , y_train)\n",
    "    \n",
    "    y_pred = p_svm.predict(X_test_generated)\n",
    "    accuracy_reslts_svm_g.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_svm_g.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_svm_g.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_svm_g.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for svm, using contextual embedding 64 vectors with difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_svm_g))\n",
    "print(\"Average of percision: \", np.average(precision_results_svm_g))\n",
    "print(\"Average of recall: \", np.average(recall_results_svm_g))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_svm_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "40caf347-fd24-4ade-bfef-b0b81835fafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:07<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for svm, using bert 768 vectors with difstoryGen\n",
      "Average  of accuracy:  0.8405987394957984\n",
      "Average of percision:  0.7976229726526894\n",
      "Average of recall:  0.8405987394957984\n",
      "Average of f1 score:  0.8119404303993726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#SVM classification with difstorygen based on contextual bert (768) document embedding\n",
    "\n",
    "accuracy_reslts_svm_c768 = []\n",
    "precision_results_svm_c768 = []\n",
    "recall_results_svm_c768 = []\n",
    "f1_results_svm_c768 = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in list(generated_contexual_dict_768.keys())[:2000]]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in list(generated_contexual_dict_768.keys())[2000:]]  # list of categories based on MeSH terms\n",
    "    \n",
    "    p_svm_c768 = Pipeline([('Normalizing',MinMaxScaler()),('SVM',svm.SVC())])\n",
    "    \n",
    "    p_svm_c768.fit(X_train_generated_768, y_train)\n",
    "    \n",
    "    y_pred = p_svm_c768.predict(X_test_generated_768)\n",
    "    \n",
    "    accuracy_reslts_svm_c768.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_svm_c768.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_svm_c768.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_svm_c768.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for svm, using bert 768 vectors with difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_svm_c768))\n",
    "print(\"Average of percision: \", np.average(precision_results_svm_c768))\n",
    "print(\"Average of recall: \", np.average(recall_results_svm_c768))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_svm_c768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4169a934-ae8c-4514-9513-5c0edb16ff9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 14.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for KNeighborsClassifier, using contextual embedding vectors without difstoryGen\n",
      "Average  of accuracy:  0.7956118771096745\n",
      "Average of percision:  0.7814267691717725\n",
      "Average of recall:  0.7956118771096745\n",
      "Average of f1 score:  0.7858529274219761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on KNeighborsClassifier\n",
    "accuracy_reslts_KNeighborsClassifier_c = []\n",
    "precision_results_KNeighborsClassifier_c = []\n",
    "recall_results_KNeighborsClassifier_c = []\n",
    "f1_results_KNeighborsClassifier_c = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in ids_for_training_basedlines]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in ids_for_test_basedlines]  # list of categories based on MeSH terms\n",
    "    \n",
    "    p_KNeighborsClassifier= Pipeline([('Normalizing',MinMaxScaler()),('KNeighborsClassifier',KNeighborsClassifier(n_neighbors=3))])\n",
    "    \n",
    "    p_KNeighborsClassifier.fit(X_train_orig, y_train)\n",
    "    \n",
    "    y_pred = p_KNeighborsClassifier.predict(X_test_orig)\n",
    "    accuracy_reslts_KNeighborsClassifier_c.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_KNeighborsClassifier_c.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_KNeighborsClassifier_c.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_KNeighborsClassifier_c.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for KNeighborsClassifier, using contextual embedding vectors without difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_KNeighborsClassifier_c))\n",
    "print(\"Average of percision: \", np.average(precision_results_KNeighborsClassifier_c))\n",
    "print(\"Average of recall: \", np.average(recall_results_KNeighborsClassifier_c))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_KNeighborsClassifier_c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "59b2634e-61b5-46c3-baa9-3d25a6db3a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:01<00:00, 10.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for KNeighborsClassifier, using contextual embedding 64 vectors with difstoryGen\n",
      "Average  of accuracy:  0.7957263001315864\n",
      "Average of percision:  0.7818894279998526\n",
      "Average of recall:  0.7957263001315864\n",
      "Average of f1 score:  0.7861866197656456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on KNeighborsClassifier for bert 64 with difstorygen\n",
    "\n",
    "accuracy_reslts_KNeighborsClassifier_g = []\n",
    "precision_results_KNeighborsClassifier_g = []\n",
    "recall_results_KNeighborsClassifier_g = []\n",
    "f1_results_KNeighborsClassifier_g = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in ids_for_training_basedlines]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in ids_for_test_basedlines]  # list of categories based on MeSH terms\n",
    "    p_KNeighborsClassifier_g = Pipeline([('Normalizing',MinMaxScaler()),('KNeighborsClassifier',KNeighborsClassifier(n_neighbors=3))])\n",
    "    \n",
    "    \n",
    "    p_KNeighborsClassifier_g.fit(X_train_generated , y_train)\n",
    "    \n",
    "    y_pred = p_KNeighborsClassifier_g.predict(X_test_generated)\n",
    "    accuracy_reslts_KNeighborsClassifier_g.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_KNeighborsClassifier_g.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_KNeighborsClassifier_g.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_KNeighborsClassifier_g.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for KNeighborsClassifier, using contextual embedding 64 vectors with difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_KNeighborsClassifier_g))\n",
    "print(\"Average of percision: \", np.average(precision_results_KNeighborsClassifier_g))\n",
    "print(\"Average of recall: \", np.average(recall_results_KNeighborsClassifier_g))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_KNeighborsClassifier_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4c8bbdb0-72e5-4888-a0a2-cfa07a565c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 15.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for KNeighborsClassifier, using bert 768 vectors with difstoryGen\n",
      "Average  of accuracy:  0.7922794117647057\n",
      "Average of percision:  0.7753753203349091\n",
      "Average of recall:  0.7922794117647057\n",
      "Average of f1 score:  0.7792481901446606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on KNeighborsClassifier for bert 768 with difstorygen\n",
    "accuracy_reslts_KNeighborsClassifier_c768 = []\n",
    "precision_results_KNeighborsClassifier_c768 = []\n",
    "recall_results_KNeighborsClassifier_c768 = []\n",
    "f1_results_KNeighborsClassifier_c768 = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in list(generated_contexual_dict_768.keys())[:2000]]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in list(generated_contexual_dict_768.keys())[2000:]]  # list of categories based on MeSH terms\n",
    "    \n",
    "    p_KNeighborsClassifier_c768 = Pipeline([('Normalizing',MinMaxScaler()),('KNeighborsClassifier',KNeighborsClassifier(n_neighbors=3))])\n",
    "\n",
    "    \n",
    "    p_KNeighborsClassifier_c768.fit(X_train_generated_768, y_train)\n",
    "    \n",
    "    y_pred = p_KNeighborsClassifier_c768.predict(X_test_generated_768)\n",
    "    \n",
    "    accuracy_reslts_KNeighborsClassifier_c768.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_KNeighborsClassifier_c768.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_KNeighborsClassifier_c768.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_KNeighborsClassifier_c768.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for KNeighborsClassifier, using bert 768 vectors with difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_KNeighborsClassifier_c768))\n",
    "print(\"Average of percision: \", np.average(precision_results_KNeighborsClassifier_c768))\n",
    "print(\"Average of recall: \", np.average(recall_results_KNeighborsClassifier_c768))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_KNeighborsClassifier_c768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f5a6e973-7f81-4519-be36-d9097f0d818f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:04<00:00,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for DecisionTreeClassifier, using contextual embedding vectors without difstoryGen\n",
      "Average  of accuracy:  0.776117054751416\n",
      "Average of percision:  0.7135251536007265\n",
      "Average of recall:  0.776117054751416\n",
      "Average of f1 score:  0.737243041425166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on Decision tree without difstorygen\n",
    "accuracy_reslts_DecisionTreeClassifier_c = []\n",
    "precision_results_DecisionTreeClassifier_c = []\n",
    "recall_results_DecisionTreeClassifier_c = []\n",
    "f1_results_DecisionTreeClassifier_c = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in ids_for_training_basedlines]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in ids_for_test_basedlines]  # list of categories based on MeSH terms\n",
    "    \n",
    "    p_DecisionTreeClassifier = Pipeline([('Normalizing',MinMaxScaler()),('DecisionTreeClassifier',DecisionTreeClassifier(criterion=\"entropy\", max_depth=3))])\n",
    "    p_DecisionTreeClassifier.fit(X_train_orig, y_train)\n",
    "    \n",
    "    y_pred = p_DecisionTreeClassifier.predict(X_test_orig)\n",
    "    accuracy_reslts_DecisionTreeClassifier_c.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_DecisionTreeClassifier_c.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_DecisionTreeClassifier_c.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_DecisionTreeClassifier_c.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for DecisionTreeClassifier, using contextual embedding vectors without difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_DecisionTreeClassifier_c))\n",
    "print(\"Average of percision: \", np.average(precision_results_DecisionTreeClassifier_c))\n",
    "print(\"Average of recall: \", np.average(recall_results_DecisionTreeClassifier_c))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_DecisionTreeClassifier_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f01fc992-7dbe-417d-a85d-4b8b034eb5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:11<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for DecisionTreeClassifier, using contextual embedding 64 vectors with difstoryGen\n",
      "Average  of accuracy:  0.775974025974026\n",
      "Average of percision:  0.6977231942028509\n",
      "Average of recall:  0.775974025974026\n",
      "Average of f1 score:  0.7275317000623819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on Decision tree with difstorygen embedding 64\n",
    "\n",
    "accuracy_reslts_DecisionTreeClassifier_g = []\n",
    "precision_results_DecisionTreeClassifier_g = []\n",
    "recall_results_DecisionTreeClassifier_g = []\n",
    "f1_results_DecisionTreeClassifier_g = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in ids_for_training_basedlines]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in ids_for_test_basedlines]  # list of categories based on MeSH terms\n",
    "    p_DecisionTreeClassifier_g = Pipeline([('Normalizing',MinMaxScaler()),('DecisionTreeClassifier',DecisionTreeClassifier(criterion=\"entropy\", max_depth=3))])\n",
    "    \n",
    "    p_DecisionTreeClassifier_g.fit(X_train_generated , y_train)\n",
    "    \n",
    "    y_pred = p_DecisionTreeClassifier_g.predict(X_test_generated)\n",
    "    accuracy_reslts_DecisionTreeClassifier_g.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_DecisionTreeClassifier_g.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_DecisionTreeClassifier_g.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_DecisionTreeClassifier_g.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for DecisionTreeClassifier, using contextual embedding 64 vectors with difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_DecisionTreeClassifier_g))\n",
    "print(\"Average of percision: \", np.average(precision_results_DecisionTreeClassifier_g))\n",
    "print(\"Average of recall: \", np.average(recall_results_DecisionTreeClassifier_g))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_DecisionTreeClassifier_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9765ba60-2738-4e17-9f58-49881eaf7e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:07<00:00,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for DecisionTreeClassifier, using bert 768 vectors with difstoryGen\n",
      "Average  of accuracy:  0.7851890756302521\n",
      "Average of percision:  0.7685538295023456\n",
      "Average of recall:  0.7851890756302521\n",
      "Average of f1 score:  0.7624755372459633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on Decision tree with difstorygen embedding 768\n",
    "accuracy_reslts_DecisionTreeClassifier_c768 = []\n",
    "precision_results_DecisionTreeClassifier_c768 = []\n",
    "recall_results_DecisionTreeClassifier_c768 = []\n",
    "f1_results_DecisionTreeClassifier_c768 = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in list(generated_contexual_dict_768.keys())[:2000]]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in list(generated_contexual_dict_768.keys())[2000:]]  # list of categories based on MeSH terms\n",
    "    \n",
    "    p_orig = Pipeline([('Normalizing',MinMaxScaler()),('DecisionTreeClassifier',DecisionTreeClassifier(criterion=\"entropy\", max_depth=3))])\n",
    "    p_orig.fit(X_train_generated_768, y_train)\n",
    "    \n",
    "    y_pred = p_orig.predict(X_test_generated_768)\n",
    "    accuracy_reslts_DecisionTreeClassifier_c768.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_DecisionTreeClassifier_c768.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_DecisionTreeClassifier_c768.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_DecisionTreeClassifier_c768.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for DecisionTreeClassifier, using bert 768 vectors with difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_DecisionTreeClassifier_c768))\n",
    "print(\"Average of percision: \", np.average(precision_results_DecisionTreeClassifier_c768))\n",
    "print(\"Average of recall: \", np.average(recall_results_DecisionTreeClassifier_c768))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_DecisionTreeClassifier_c768))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e30eff09-0b3a-4c28-8735-049134144b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:01<00:00,  7.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for LogisticRegression, using contextual embedding vectors without difstoryGen\n",
      "Average  of accuracy:  0.8338577721837634\n",
      "Average of percision:  0.8199932312861806\n",
      "Average of recall:  0.8338577721837634\n",
      "Average of f1 score:  0.8172561161753802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on LogisticRegression()\n",
    "accuracy_reslts_LogisticRegression_c = []\n",
    "precision_results_LogisticRegression_c = []\n",
    "recall_results_LogisticRegression_c = []\n",
    "f1_results_LogisticRegression_c = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in ids_for_training_basedlines]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in ids_for_test_basedlines]  # list of categories based on MeSH terms\n",
    "    \n",
    "    p_LogisticRegression = Pipeline([('Normalizing',MinMaxScaler()),('LogisticRegression',LogisticRegression(solver='lbfgs', max_iter=1000))])\n",
    "    p_LogisticRegression.fit(X_train_orig, y_train)\n",
    "    \n",
    "    y_pred = p_LogisticRegression.predict(X_test_orig)\n",
    "    accuracy_reslts_LogisticRegression_c.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_LogisticRegression_c.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_LogisticRegression_c.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_LogisticRegression_c.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for LogisticRegression, using contextual embedding vectors without difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_LogisticRegression_c))\n",
    "print(\"Average of percision: \", np.average(precision_results_LogisticRegression_c))\n",
    "print(\"Average of recall: \", np.average(recall_results_LogisticRegression_c))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_LogisticRegression_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4f122b47-ab44-486c-94e9-574d23d39469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:07<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for LogisticRegression, using contextual embedding 64 vectors with difstoryGen\n",
      "Average  of accuracy:  0.8343583729046283\n",
      "Average of percision:  0.8212390285304422\n",
      "Average of recall:  0.8343583729046283\n",
      "Average of f1 score:  0.8177816168521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on LogisticRegression for bert 64\n",
    "\n",
    "accuracy_reslts_LogisticRegression_g = []\n",
    "precision_results_LogisticRegression_g = []\n",
    "recall_results_LogisticRegression_g = []\n",
    "f1_results_LogisticRegression_g = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in ids_for_training_basedlines]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in ids_for_test_basedlines]  # list of categories based on MeSH terms\n",
    "    p_LogisticRegression_g = Pipeline([('Normalizing',MinMaxScaler()),('LogisticRegression',LogisticRegression(solver='lbfgs', max_iter=1000))])\n",
    "    \n",
    "    p_LogisticRegression_g.fit(X_train_generated , y_train)\n",
    "    \n",
    "    y_pred = p_LogisticRegression_g.predict(X_test_generated)\n",
    "    accuracy_reslts_LogisticRegression_g.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_LogisticRegression_g.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_LogisticRegression_g.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_LogisticRegression_g.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for LogisticRegression, using contextual embedding 64 vectors with difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_LogisticRegression_g))\n",
    "print(\"Average of percision: \", np.average(precision_results_LogisticRegression_g))\n",
    "print(\"Average of recall: \", np.average(recall_results_LogisticRegression_g))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_LogisticRegression_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "685f6128-fd0c-4ff9-8f8a-d87615703ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:02<00:00,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for LogisticRegression_c768, using bert 768 vectors with difstoryGen\n",
      "Average  of accuracy:  0.8432247899159664\n",
      "Average of percision:  0.8342457710971877\n",
      "Average of recall:  0.8432247899159664\n",
      "Average of f1 score:  0.8369003770788215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on LogisticRegression for bert 768\n",
    "accuracy_reslts_LogisticRegression_c768 = []\n",
    "precision_results_LogisticRegression_c768 = []\n",
    "recall_results_LogisticRegression_c768 = []\n",
    "f1_results_LogisticRegression_c768 = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in list(generated_contexual_dict_768.keys())[:2000]]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in list(generated_contexual_dict_768.keys())[2000:]]  # list of categories based on MeSH terms\n",
    "    \n",
    "    p_LogisticRegression_c768 = Pipeline([('Normalizing',MinMaxScaler()),('LogisticRegression',LogisticRegression(solver='lbfgs', max_iter=1000))])\n",
    "    p_LogisticRegression_c768.fit(X_train_generated_768, y_train)\n",
    "    \n",
    "    y_pred = p_LogisticRegression_c768.predict(X_test_generated_768)\n",
    "    accuracy_reslts_LogisticRegression_c768.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_LogisticRegression_c768.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_LogisticRegression_c768.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_LogisticRegression_c768.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for LogisticRegression_c768, using bert 768 vectors with difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_LogisticRegression_c768))\n",
    "print(\"Average of percision: \", np.average(precision_results_LogisticRegression_c768))\n",
    "print(\"Average of recall: \", np.average(recall_results_LogisticRegression_c768))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_LogisticRegression_c768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "8d033885-2a49-4610-8fa4-82b43da207c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 34.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for p_GaussianNB, using contextual embedding vectors without difstoryGen\n",
      "Average  of accuracy:  0.7916499799759712\n",
      "Average of percision:  0.795747832689037\n",
      "Average of recall:  0.7916499799759712\n",
      "Average of f1 score:  0.7929831585908256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on GaussianNB\n",
    "accuracy_reslts_GaussianNB_c = []\n",
    "precision_results_GaussianNB_c = []\n",
    "recall_results_GaussianNB_c = []\n",
    "f1_results_GaussianNB_c = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in ids_for_training_basedlines]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in ids_for_test_basedlines]  # list of categories based on MeSH terms\n",
    "    \n",
    "    p_GaussianNB = Pipeline([('Normalizing',MinMaxScaler()),('GaussianNB',GaussianNB())])\n",
    "    p_GaussianNB.fit(X_train_orig, y_train)\n",
    "    \n",
    "    y_pred = p_GaussianNB.predict(X_test_orig)\n",
    "    accuracy_reslts_GaussianNB_c.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_GaussianNB_c.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_GaussianNB_c.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_GaussianNB_c.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for p_GaussianNB, using contextual embedding vectors without difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_GaussianNB_c))\n",
    "print(\"Average of percision: \", np.average(precision_results_GaussianNB_c))\n",
    "print(\"Average of recall: \", np.average(recall_results_GaussianNB_c))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_GaussianNB_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "838868d8-8831-4956-8a0a-169726d5f60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 32.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for GaussianNB, using contextual embedding 64 vectors with difstoryGen\n",
      "Average  of accuracy:  0.697179472509869\n",
      "Average of percision:  0.7570874998417961\n",
      "Average of recall:  0.697179472509869\n",
      "Average of f1 score:  0.7010706211056047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on GaussianNB with difstorygen bert 64\n",
    "\n",
    "accuracy_reslts_GaussianNB_g = []\n",
    "precision_results_GaussianNB_g = []\n",
    "recall_results_GaussianNB_g = []\n",
    "f1_results_GaussianNB_g = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in ids_for_training_basedlines]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in ids_for_test_basedlines]  # list of categories based on MeSH terms\n",
    "    p_GaussianNB_g = Pipeline([('Normalizing',MinMaxScaler()),('GaussianNB',GaussianNB())])\n",
    "    \n",
    "    p_GaussianNB_g.fit(X_train_generated , y_train)\n",
    "    \n",
    "    y_pred = p_GaussianNB_g.predict(X_test_generated)\n",
    "    accuracy_reslts_GaussianNB_g.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_GaussianNB_g.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_GaussianNB_g.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_GaussianNB_g.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for GaussianNB, using contextual embedding 64 vectors with difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_GaussianNB_g))\n",
    "print(\"Average of percision: \", np.average(precision_results_GaussianNB_g))\n",
    "print(\"Average of recall: \", np.average(recall_results_GaussianNB_g))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_GaussianNB_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "deaeb97f-b4ff-4d05-8fdb-dc5ca457e2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 55.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for GaussianNB, using bert 768 vectors with difstoryGen\n",
      "Average  of accuracy:  0.7322741596638656\n",
      "Average of percision:  0.7990654883234248\n",
      "Average of recall:  0.7322741596638656\n",
      "Average of f1 score:  0.753926579547728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on GaussianNB with difstorygen bert 768\n",
    "accuracy_reslts_GaussianNB_c768 = []\n",
    "precision_results_GaussianNB_c768 = []\n",
    "recall_results_GaussianNB_c768 = []\n",
    "f1_results_GaussianNB_c768 = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in list(generated_contexual_dict_768.keys())[:2000]]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in list(generated_contexual_dict_768.keys())[2000:]]  # list of categories based on MeSH terms\n",
    "    \n",
    "    p_GaussianNB_c768 = Pipeline([('Normalizing',MinMaxScaler()),('GaussianNB',GaussianNB())])\n",
    "    p_GaussianNB_c768.fit(X_train_generated_768, y_train)\n",
    "    \n",
    "    y_pred = p_GaussianNB_c768.predict(X_test_generated_768)\n",
    "    accuracy_reslts_GaussianNB_c768.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_GaussianNB_c768.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_GaussianNB_c768.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_GaussianNB_c768.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for GaussianNB, using bert 768 vectors with difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_GaussianNB_c768))\n",
    "print(\"Average of percision: \", np.average(precision_results_GaussianNB_c768))\n",
    "print(\"Average of recall: \", np.average(recall_results_GaussianNB_c768))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_GaussianNB_c768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "aec7fc06-4981-493a-92aa-3efbfc7c3606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "b8860f2d-3298-4d5e-be88-50ae1dc9596a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:01<00:00, 11.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for SGDClassifier, using contextual embedding vectors without difstoryGen\n",
      "Average  of accuracy:  0.8288660678528521\n",
      "Average of percision:  0.7898654865956052\n",
      "Average of recall:  0.8288660678528521\n",
      "Average of f1 score:  0.8018011659427879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on SGDClassifier\n",
    "accuracy_reslts_SGDClassifier_c = []\n",
    "precision_results_SGDClassifier_c = []\n",
    "recall_results_SGDClassifier_c = []\n",
    "f1_results_SGDClassifier_c = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in ids_for_training_basedlines]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in ids_for_test_basedlines]  # list of categories based on MeSH terms\n",
    "    \n",
    "    p_SGDClassifier = Pipeline([('Normalizing',MinMaxScaler()),('SGDClassifier',SGDClassifier(learning_rate='optimal', max_iter=1000, tol=1e-3))])\n",
    "    p_SGDClassifier.fit(X_train_orig, y_train)\n",
    "    \n",
    "    y_pred = p_SGDClassifier.predict(X_test_orig)\n",
    "    accuracy_reslts_SGDClassifier_c.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_SGDClassifier_c.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_SGDClassifier_c.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_SGDClassifier_c.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for SGDClassifier, using contextual embedding vectors without difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_SGDClassifier_c))\n",
    "print(\"Average of percision: \", np.average(precision_results_SGDClassifier_c))\n",
    "print(\"Average of recall: \", np.average(recall_results_SGDClassifier_c))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_SGDClassifier_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "e169572c-995b-4fc6-b0d7-17fe93079947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:03<00:00,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for SGDClassifier, using contextual embedding 64 vectors with difstoryGen\n",
      "Average  of accuracy:  0.8276646261227759\n",
      "Average of percision:  0.8071243099843535\n",
      "Average of recall:  0.8276646261227759\n",
      "Average of f1 score:  0.8038232711716501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on SGDClassifier for bert 64 with difstorygen\n",
    "\n",
    "accuracy_reslts_SGDClassifier_g = []\n",
    "precision_results_SGDClassifier_g = []\n",
    "recall_results_SGDClassifier_g = []\n",
    "f1_results_SGDClassifier_g = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in ids_for_training_basedlines]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in ids_for_test_basedlines]  # list of categories based on MeSH terms\n",
    "    p_SGDClassifier_g = Pipeline([('Normalizing',MinMaxScaler()),('SGDClassifier',SGDClassifier(learning_rate='optimal', max_iter=1000, tol=1e-3))])\n",
    "    \n",
    "    p_SGDClassifier_g.fit(X_train_generated , y_train)\n",
    "    \n",
    "    y_pred = p_SGDClassifier_g.predict(X_test_generated)\n",
    "    accuracy_reslts_SGDClassifier_g.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_SGDClassifier_g.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_SGDClassifier_g.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_SGDClassifier_g.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for SGDClassifier, using contextual embedding 64 vectors with difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_SGDClassifier_g))\n",
    "print(\"Average of percision: \", np.average(precision_results_SGDClassifier_g))\n",
    "print(\"Average of recall: \", np.average(recall_results_SGDClassifier_g))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_SGDClassifier_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "74944361-afdf-469c-8843-86719a6fa2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:03<00:00,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for naive bayes, using bert 768 vectors with difstoryGen\n",
      "Average  of accuracy:  0.8233981092436976\n",
      "Average of percision:  0.8243457521034975\n",
      "Average of recall:  0.8233981092436976\n",
      "Average of f1 score:  0.8147744971219907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on SGDClassifier for bert 768 with difstorygen\n",
    "accuracy_reslts_SGDClassifier_c768 = []\n",
    "precision_results_SGDClassifier_c768 = []\n",
    "recall_results_SGDClassifier_c768 = []\n",
    "f1_results_SGDClassifier_c768 = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in list(generated_contexual_dict_768.keys())[:2000]]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in list(generated_contexual_dict_768.keys())[2000:]]  # list of categories based on MeSH terms\n",
    "    \n",
    "    p_SGDClassifier_c768 = Pipeline([('Normalizing',MinMaxScaler()),('SGDClassifier',SGDClassifier(learning_rate='optimal',max_iter=1000))])\n",
    "    p_SGDClassifier_c768.fit(X_train_generated_768, y_train)\n",
    "    \n",
    "    y_pred = p_SGDClassifier_c768.predict(X_test_generated_768)\n",
    "    accuracy_reslts_SGDClassifier_c768.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_SGDClassifier_c768.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_SGDClassifier_c768.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_SGDClassifier_c768.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for SGDClassifier, using bert 768 vectors with difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_SGDClassifier_c768))\n",
    "print(\"Average of percision: \", np.average(precision_results_SGDClassifier_c768))\n",
    "print(\"Average of recall: \", np.average(recall_results_SGDClassifier_c768))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_SGDClassifier_c768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ac105032-4472-4406-913f-841e63a99b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "007c2426-1e8a-4c6f-aee0-b790f5e5b4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:00<00:00,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for RandomForestClassifier, using contextual embedding vectors without difstoryGen\n",
      "Average  of accuracy:  0.8219577779049144\n",
      "Average of percision:  0.8113334876281909\n",
      "Average of recall:  0.8219577779049144\n",
      "Average of f1 score:  0.7942972923930105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on RandomForestClassifier\n",
    "accuracy_reslts_RandomForestClassifier_c = []\n",
    "precision_results_RandomForestClassifier_c = []\n",
    "recall_results_RandomForestClassifier_c = []\n",
    "f1_results_RandomForestClassifier_c = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in ids_for_training_basedlines]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in ids_for_test_basedlines]  # list of categories based on MeSH terms\n",
    "    \n",
    "    p_RandomForestClassifier = Pipeline([('Normalizing',MinMaxScaler()),('RandomForestClassifier',RandomForestClassifier(n_estimators =100))])\n",
    "    p_RandomForestClassifier.fit(X_train_orig, y_train)\n",
    "    \n",
    "    y_pred = p_RandomForestClassifier.predict(X_test_orig)\n",
    "    accuracy_reslts_RandomForestClassifier_c.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_RandomForestClassifier_c.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_RandomForestClassifier_c.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_RandomForestClassifier_c.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for RandomForestClassifier, using contextual embedding vectors without difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_RandomForestClassifier_c))\n",
    "print(\"Average of percision: \", np.average(precision_results_RandomForestClassifier_c))\n",
    "print(\"Average of recall: \", np.average(recall_results_RandomForestClassifier_c))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_RandomForestClassifier_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "5fbaaa5c-c88c-44b3-8abd-dd126199e9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [03:40<00:00, 15.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for RandomForestClassifier, using contextual embedding 64 vectors with difstoryGen\n",
      "Average  of accuracy:  0.816451169975399\n",
      "Average of percision:  0.8055700964901785\n",
      "Average of recall:  0.816451169975399\n",
      "Average of f1 score:  0.7870176422299624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on RandomForestClassifier with difstorygen with bert 64\n",
    "\n",
    "accuracy_reslts_RandomForestClassifier_g = []\n",
    "precision_results_RandomForestClassifier_g = []\n",
    "recall_results_RandomForestClassifier_g = []\n",
    "f1_results_RandomForestClassifier_g = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in ids_for_training_basedlines]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in ids_for_test_basedlines]  # list of categories based on MeSH terms\n",
    "    p_RandomForestClassifier_g = Pipeline([('Normalizing',MinMaxScaler()),('RandomForestClassifier',RandomForestClassifier(n_estimators =100))])\n",
    "    \n",
    "    p_RandomForestClassifier_g.fit(X_train_generated , y_train)\n",
    "    \n",
    "    y_pred = p_RandomForestClassifier_g.predict(X_test_generated)\n",
    "    accuracy_reslts_RandomForestClassifier_g.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_RandomForestClassifier_g.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_RandomForestClassifier_g.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_RandomForestClassifier_g.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for RandomForestClassifier, using contextual embedding 64 vectors with difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_RandomForestClassifier_g))\n",
    "print(\"Average of percision: \", np.average(precision_results_RandomForestClassifier_g))\n",
    "print(\"Average of recall: \", np.average(recall_results_RandomForestClassifier_g))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_RandomForestClassifier_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e126af2c-53a4-4b49-b92e-50fa7fbf8560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:38<00:00,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for RandomForestClassifier, using bert 768 vectors with difstoryGen\n",
      "Average  of accuracy:  0.8247111344537815\n",
      "Average of percision:  0.8116804740587625\n",
      "Average of recall:  0.8247111344537815\n",
      "Average of f1 score:  0.7983613254288996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on RandomForestClassifier with difstorygen with bert 768\n",
    "accuracy_reslts_RandomForestClassifier_c768 = []\n",
    "precision_results_RandomForestClassifier_c768 = []\n",
    "recall_results_RandomForestClassifier_c768 = []\n",
    "f1_results_RandomForestClassifier_c768 = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in list(generated_contexual_dict_768.keys())[:2000]]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in list(generated_contexual_dict_768.keys())[2000:]]  # list of categories based on MeSH terms\n",
    "    \n",
    "    p_RandomForestClassifier_c768 = Pipeline([('Normalizing',MinMaxScaler()),('RandomForestClassifier',RandomForestClassifier(n_estimators =100))])\n",
    "    p_RandomForestClassifier_c768.fit(X_train_generated_768, y_train)\n",
    "    \n",
    "    y_pred = p_RandomForestClassifier_c768.predict(X_test_generated_768)\n",
    "    accuracy_reslts_RandomForestClassifier_c768.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_RandomForestClassifier_c768.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_RandomForestClassifier_c768.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_RandomForestClassifier_c768.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for RandomForestClassifier, using bert 768 vectors with difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_RandomForestClassifier_c768))\n",
    "print(\"Average of percision: \", np.average(precision_results_RandomForestClassifier_c768))\n",
    "print(\"Average of recall: \", np.average(recall_results_RandomForestClassifier_c768))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_RandomForestClassifier_c768))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "b35f1a48-1cf4-4b23-ab2e-22411defc554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "1e346828-c386-43cb-8867-96440837da4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:43<00:00,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for GradientBoostingClassifier, using contextual embedding vectors without difstoryGen\n",
      "Average  of accuracy:  0.8244607815092396\n",
      "Average of percision:  0.8100160003168922\n",
      "Average of recall:  0.8244607815092396\n",
      "Average of f1 score:  0.8101287018228306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on GradientBoostingClassifier\n",
    "accuracy_reslts_GradientBoostingClassifier_c = []\n",
    "precision_results_GradientBoostingClassifier_c = []\n",
    "recall_results_GradientBoostingClassifier_c = []\n",
    "f1_results_GradientBoostingClassifier_c = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in ids_for_training_basedlines]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in ids_for_test_basedlines]  # list of categories based on MeSH terms\n",
    "    \n",
    "    p_GradientBoostingClassifier = Pipeline([('Normalizing',MinMaxScaler()),('GradientBoostingClassifier',GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1))])\n",
    "    p_GradientBoostingClassifier.fit(X_train_orig, y_train)\n",
    "    \n",
    "    y_pred = p_GradientBoostingClassifier.predict(X_test_orig)\n",
    "    accuracy_reslts_GradientBoostingClassifier_c.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_GradientBoostingClassifier_c.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_GradientBoostingClassifier_c.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_GradientBoostingClassifier_c.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for GradientBoostingClassifier, using contextual embedding vectors without difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_GradientBoostingClassifier_c))\n",
    "print(\"Average of percision: \", np.average(precision_results_GradientBoostingClassifier_c))\n",
    "print(\"Average of recall: \", np.average(recall_results_GradientBoostingClassifier_c))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_GradientBoostingClassifier_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "a9693a56-c43b-47c4-922e-b0ef314ba7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [04:25<00:00, 18.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for GradientBoostingClassifier, using contextual embedding 64 vectors with difstoryGen\n",
      "Average  of accuracy:  0.8239887865438525\n",
      "Average of percision:  0.8091933284376933\n",
      "Average of recall:  0.8239887865438525\n",
      "Average of f1 score:  0.809696693437858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on GradientBoostingClassifier with difstoryGen and bert 64\n",
    "\n",
    "accuracy_reslts_GradientBoostingClassifier_g = []\n",
    "precision_results_GradientBoostingClassifier_g = []\n",
    "recall_results_GradientBoostingClassifier_g = []\n",
    "f1_results_GradientBoostingClassifier_g = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in ids_for_training_basedlines]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in ids_for_test_basedlines]  # list of categories based on MeSH terms\n",
    "    p_GradientBoostingClassifier_g = Pipeline([('Normalizing',MinMaxScaler()),('GradientBoostingClassifier',GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1))])\n",
    "    \n",
    "    p_GradientBoostingClassifier_g.fit(X_train_generated , y_train)\n",
    "    \n",
    "    y_pred = p_GradientBoostingClassifier_g.predict(X_test_generated)\n",
    "    accuracy_reslts_GradientBoostingClassifier_g.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_GradientBoostingClassifier_g.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_GradientBoostingClassifier_g.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_GradientBoostingClassifier_g.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for GradientBoostingClassifier, using contextual embedding 64 vectors with difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_GradientBoostingClassifier_g))\n",
    "print(\"Average of percision: \", np.average(precision_results_GradientBoostingClassifier_g))\n",
    "print(\"Average of recall: \", np.average(recall_results_GradientBoostingClassifier_g))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_GradientBoostingClassifier_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "1d2d7cbd-9436-41c8-8809-3f61770ccce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:47<00:00, 11.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for GradientBoostingClassifier, using bert 768 vectors with difstoryGen\n",
      "Average  of accuracy:  0.7912289915966387\n",
      "Average of percision:  0.7832770882505814\n",
      "Average of recall:  0.7912289915966387\n",
      "Average of f1 score:  0.7867078084546495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classification based on GradientBoostingClassifier with difstoryGen and bert 768\n",
    "\n",
    "accuracy_reslts_GradientBoostingClassifier_c768 = []\n",
    "precision_results_GradientBoostingClassifier_c768 = []\n",
    "recall_results_GradientBoostingClassifier_c768 = []\n",
    "f1_results_GradientBoostingClassifier_c768 = []\n",
    "for r in tqdm(range(label_size)):\n",
    "    # Example training data\n",
    "    y_train = [label_dict[i][r] for i in list(generated_contexual_dict_768.keys())[:2000]]  # list of categories based on MeSH terms\n",
    "    y_test = [label_dict[i][r] for i in list(generated_contexual_dict_768.keys())[2000:]]  # list of categories based on MeSH terms\n",
    "    \n",
    "    p_GradientBoostingClassifier_c768 = Pipeline([('Normalizing',MinMaxScaler()),('GradientBoostingClassifier',GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1))])\n",
    "    p_GradientBoostingClassifier_c768.fit(X_train_generated_768, y_train)\n",
    "    \n",
    "    y_pred = p_GradientBoostingClassifier_c768.predict(X_test_generated_768)\n",
    "    accuracy_reslts_GradientBoostingClassifier_c768.append(accuracy_score(y_test, y_pred))\n",
    "    precision_results_GradientBoostingClassifier_c768.append(precision_score(y_test, y_pred, zero_division=0, average='weighted'))\n",
    "    recall_results_GradientBoostingClassifier_c768.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_results_GradientBoostingClassifier_c768.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"results for GradientBoostingClassifier, using bert 768 vectors with difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts_GradientBoostingClassifier_c768))\n",
    "print(\"Average of percision: \", np.average(precision_results_GradientBoostingClassifier_c768))\n",
    "print(\"Average of recall: \", np.average(recall_results_GradientBoostingClassifier_c768))\n",
    "print(\"Average of f1 score: \", np.average(f1_results_GradientBoostingClassifier_c768))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa6f1c-01ce-42a3-97da-ed860374695a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed770c92-81ad-4ea2-afa3-98b87a3359ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ab441f-8e18-4fc5-8230-d14a6968c1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca9017-4fd6-4e42-9dd3-f92525d2d91a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c8290-7eb5-49f5-ab4b-7114bdb13a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf43266a-5519-4663-9695-443851e954d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61ae0f15-0975-4025-8400-5277496586c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d664afd-7dae-4d58-8382-89e4c4cabe0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458dc076-ade5-47ae-84f2-77b0fc61abe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aa39a0-7172-4f9e-9e52-6ae40fc3e727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66be63d0-6559-498d-8300-87184d2205bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de3b7f-98a4-4bac-8601-1230cb5cb5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3bdd80-c5d1-428a-bc29-8a75912f54c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595bff48-267b-437f-ad8f-94c735fffbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f64d3ce-0c90-4178-8f09-04402e77cb1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80512d-c31b-435c-99d7-fbfcc2aef5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91b3420-a129-4cdb-8604-e2afb679d194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9982c2-1552-4d14-b347-80422c86a03a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151605c9-6714-4912-a7cf-215657102a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775703c6-8b11-4307-bd91-f15054fd25fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515e7c9f-f46d-437f-8207-4bc4560f0dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a67322-5379-463c-876c-3665c3cf94b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad8836-c71c-400e-ba20-a480a47d2473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad694be-88ae-4e24-82d5-6615a8cbf0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7845ebcc-75f4-476e-a0d5-e7d84bee14e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305a9b52-f0ca-497f-9eb1-b334abf7dc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e254977b-2b63-4cad-8861-f91c62bc37f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcac9b3-796f-4283-a954-523313116bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1f13daee-d66d-433b-91af-00cf78a596f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643f9e8-4065-442c-808f-e3c4f7d162c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf2e1c-a02d-480e-a1da-eed74ac26e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df0b12-165c-4ab4-a0bc-a12da99c1a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2b86342d-e37b-45a8-9304-145744d631c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 2s 35ms/step - loss: 0.6944 - accuracy: 0.5092\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.6878 - accuracy: 0.5452\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.6823 - accuracy: 0.5840\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.6822 - accuracy: 0.5614\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 29ms/step - loss: 0.6613 - accuracy: 0.6109\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 28ms/step - loss: 0.6756 - accuracy: 0.5939\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 29ms/step - loss: 0.6575 - accuracy: 0.6349\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 27ms/step - loss: 0.6610 - accuracy: 0.6158\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 27ms/step - loss: 0.6799 - accuracy: 0.5946\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.6491 - accuracy: 0.6088\n",
      "8/8 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/14 [00:13<02:51, 13.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 2s 37ms/step - loss: 0.2226 - accuracy: 0.9449\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 25ms/step - loss: 0.1801 - accuracy: 0.9576\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.1809 - accuracy: 0.9576\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.1762 - accuracy: 0.9576\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 0.1762 - accuracy: 0.9576\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 0.1781 - accuracy: 0.9576\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 24ms/step - loss: 0.1749 - accuracy: 0.9576\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 0.1741 - accuracy: 0.9576\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 24ms/step - loss: 0.1725 - accuracy: 0.9576\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.1723 - accuracy: 0.9576\n",
      "8/8 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 2/14 [00:24<02:27, 12.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 2s 32ms/step - loss: 0.6941 - accuracy: 0.5360\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 29ms/step - loss: 0.6821 - accuracy: 0.5749\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 0.6749 - accuracy: 0.5855\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 0.6780 - accuracy: 0.5671\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.6781 - accuracy: 0.5664\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.6535 - accuracy: 0.6278\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.6579 - accuracy: 0.6116\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.6345 - accuracy: 0.6582\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.6148 - accuracy: 0.6751\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.5741 - accuracy: 0.7069\n",
      "8/8 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 3/14 [00:36<02:11, 11.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 2s 32ms/step - loss: 0.6357 - accuracy: 0.6744\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 28ms/step - loss: 0.6291 - accuracy: 0.6758\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.5898 - accuracy: 0.6815\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.5480 - accuracy: 0.7097\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 0.5140 - accuracy: 0.7387\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 0.4891 - accuracy: 0.7571\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 0.4574 - accuracy: 0.7768\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.4408 - accuracy: 0.7888\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.4255 - accuracy: 0.8016\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.4189 - accuracy: 0.7987\n",
      "8/8 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 4/14 [00:48<01:59, 11.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 2s 35ms/step - loss: 0.4961 - accuracy: 0.8164\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.4579 - accuracy: 0.8298\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.4568 - accuracy: 0.8298\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 0.4558 - accuracy: 0.8298\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.4514 - accuracy: 0.8298\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.4556 - accuracy: 0.8298\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 0.4513 - accuracy: 0.8298\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 0.4548 - accuracy: 0.8298\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.4563 - accuracy: 0.8298\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 0.4468 - accuracy: 0.8298\n",
      "8/8 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 5/14 [00:59<01:45, 11.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 2s 32ms/step - loss: 0.4929 - accuracy: 0.8065\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 24ms/step - loss: 0.4803 - accuracy: 0.8164\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 0.4753 - accuracy: 0.8164\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 24ms/step - loss: 0.4762 - accuracy: 0.8164\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.4689 - accuracy: 0.8164\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 0.4494 - accuracy: 0.8164\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 24ms/step - loss: 0.4525 - accuracy: 0.8150\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 0.4455 - accuracy: 0.8157\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 0.4348 - accuracy: 0.8143\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.4226 - accuracy: 0.8150\n",
      "8/8 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 6/14 [01:11<01:34, 11.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 2s 36ms/step - loss: 0.6187 - accuracy: 0.6907\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 24ms/step - loss: 0.6040 - accuracy: 0.7133\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.5993 - accuracy: 0.7133\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 24ms/step - loss: 0.5982 - accuracy: 0.7133\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 24ms/step - loss: 0.6012 - accuracy: 0.7133\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.5988 - accuracy: 0.7133\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.5976 - accuracy: 0.7133\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 29ms/step - loss: 0.5897 - accuracy: 0.7133\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 29ms/step - loss: 0.5779 - accuracy: 0.7168\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 28ms/step - loss: 0.5730 - accuracy: 0.7090\n",
      "8/8 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 7/14 [01:24<01:25, 12.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 2s 40ms/step - loss: 0.3787 - accuracy: 0.8821\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 29ms/step - loss: 0.3635 - accuracy: 0.8842\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 25ms/step - loss: 0.3597 - accuracy: 0.8842\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 24ms/step - loss: 0.3634 - accuracy: 0.8842\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 24ms/step - loss: 0.3713 - accuracy: 0.8842\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 0.3606 - accuracy: 0.8842\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 24ms/step - loss: 0.3624 - accuracy: 0.8842\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 24ms/step - loss: 0.4123 - accuracy: 0.8425\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 0.3611 - accuracy: 0.8842\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.3635 - accuracy: 0.8842\n",
      "8/8 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 8/14 [01:37<01:13, 12.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 2s 34ms/step - loss: 0.3794 - accuracy: 0.8792\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.3610 - accuracy: 0.8842\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 27ms/step - loss: 0.3586 - accuracy: 0.8842\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 28ms/step - loss: 0.3630 - accuracy: 0.8842\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 28ms/step - loss: 0.3595 - accuracy: 0.8842\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.3551 - accuracy: 0.8842\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.3470 - accuracy: 0.8842\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.3387 - accuracy: 0.8842\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.3223 - accuracy: 0.8842\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.2950 - accuracy: 0.8877\n",
      "8/8 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 9/14 [01:49<01:01, 12.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 2s 34ms/step - loss: 0.3616 - accuracy: 0.8891\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.3405 - accuracy: 0.8934\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.3419 - accuracy: 0.8934\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 0.3402 - accuracy: 0.8934\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 0.3408 - accuracy: 0.8934\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 0.3400 - accuracy: 0.8934\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.3387 - accuracy: 0.8934\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.3355 - accuracy: 0.8934\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 0.3350 - accuracy: 0.8934\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 0.3331 - accuracy: 0.8934\n",
      "8/8 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 10/14 [02:00<00:47, 11.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 2s 36ms/step - loss: 0.5189 - accuracy: 0.8008\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 30ms/step - loss: 0.4949 - accuracy: 0.8058\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 0.4956 - accuracy: 0.8058\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.4939 - accuracy: 0.8058\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.5255 - accuracy: 0.7775\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.4971 - accuracy: 0.8058\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 0.4953 - accuracy: 0.8058\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.4922 - accuracy: 0.8058\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.4924 - accuracy: 0.8058\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.4910 - accuracy: 0.8058\n",
      "8/8 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 11/14 [02:11<00:34, 11.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 2s 31ms/step - loss: 0.6735 - accuracy: 0.5833\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 25ms/step - loss: 0.6220 - accuracy: 0.6688\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 0.5454 - accuracy: 0.7436\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.5072 - accuracy: 0.7613\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.4597 - accuracy: 0.7980\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.4501 - accuracy: 0.7945\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.4261 - accuracy: 0.8044\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 0.3997 - accuracy: 0.8383\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 27ms/step - loss: 0.3850 - accuracy: 0.8411\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 0.3974 - accuracy: 0.8291\n",
      "8/8 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 12/14 [02:22<00:23, 11.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 2s 30ms/step - loss: 0.6768 - accuracy: 0.5643\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.6372 - accuracy: 0.6398\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.5953 - accuracy: 0.6949\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 0.5614 - accuracy: 0.7225\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.5299 - accuracy: 0.7493\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 0.5363 - accuracy: 0.7451\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.5113 - accuracy: 0.7648\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.5055 - accuracy: 0.7620\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.5128 - accuracy: 0.7514\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 0.5077 - accuracy: 0.7705\n",
      "8/8 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 13/14 [02:33<00:11, 11.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 2s 32ms/step - loss: 0.4480 - accuracy: 0.8397\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.4232 - accuracy: 0.8538\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 33ms/step - loss: 0.4195 - accuracy: 0.8538\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 0.4136 - accuracy: 0.8538\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.4140 - accuracy: 0.8538\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 24ms/step - loss: 0.4090 - accuracy: 0.8538\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.4016 - accuracy: 0.8538\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.3850 - accuracy: 0.8552\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 0.3691 - accuracy: 0.8538\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 0.3538 - accuracy: 0.8602\n",
      "8/8 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:44<00:00, 11.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for RNN, using contextual embedding vectors without difstoryGen\n",
      "Average  of accuracy:  0.7869147659063626\n",
      "Average of percision:  0.7261407473902883\n",
      "Average of recall:  0.7869147659063626\n",
      "Average of f1 score:  0.7446993735893557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#classify by RNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X_train = [contextual_dict[i[0]] for i in total_train if i[0] in id_contextual]  # list of abstracts\n",
    "X_test = [contextual_dict[i[0]] for i in total_test if i[0] in id_contextual] \n",
    "# Assuming 'document_embeddings' is a list of embeddings and 'labels' is your list of labels\n",
    "document_embeddings = [contextual_dict[i[0]] for i in total_train if i[0] in id_contextual]\n",
    "\n",
    "accuracy_reslts = []\n",
    "precision_results = []\n",
    "recall_results = []\n",
    "f1_results = []\n",
    "for r in tqdm(range(len(total_train[0])-3)):\n",
    "    # Example training data\n",
    "    labels = [i[r+3] for i in total_train if i[0] in id_contextual ]  # list of categories based on MeSH terms\n",
    "    y_test = [i[r+3] for i in total_test if i[0] in id_contextual]  # list of categories based on MeSH terms\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(labels)\n",
    "    encoded_labels = tf.keras.utils.to_categorical(encoded_labels)\n",
    "\n",
    "    padded_docs = pad_sequences(document_embeddings, padding='post')\n",
    "    model = Sequential([\n",
    "        # Assuming embedding dimension is 100. Adjust 'input_dim' to your vocabulary size,\n",
    "        # 'output_dim' to the size of your embeddings, and 'input_length' to the length of your padded documents\n",
    "        Embedding(input_dim=len(X_train[0]), output_dim=2, input_length=padded_docs.shape[1]),\n",
    "        SimpleRNN(units=64),\n",
    "        Dense(units=len(np.unique(labels)), activation='softmax')\n",
    "    ])\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(padded_docs, encoded_labels, epochs=10, verbose=1)\n",
    "\n",
    "    new_padded_docs = pad_sequences(X_test, maxlen=padded_docs.shape[1], padding='post')\n",
    "    # Predict\n",
    "    predictions = model.predict(new_padded_docs)\n",
    "    # Decode predicted labels\n",
    "    predicted_labels = label_encoder.inverse_transform([np.argmax(pred) for pred in predictions])\n",
    "\n",
    "    accuracy_reslts.append(accuracy_score(y_test, predicted_labels))\n",
    "    precision_results.append(precision_score(y_test, predicted_labels, zero_division=0, average='weighted'))\n",
    "    recall_results.append(recall_score(y_test, predicted_labels, average='weighted'))\n",
    "    f1_results.append(f1_score(y_test, predicted_labels, average='weighted'))\n",
    "\n",
    "print(\"results for RNN, using contextual embedding vectors without difstoryGen\")\n",
    "print(\"Average  of accuracy: \", np.average(accuracy_reslts))\n",
    "print(\"Average of percision: \", np.average(precision_results))\n",
    "print(\"Average of recall: \", np.average(recall_results))\n",
    "print(\"Average of f1 score: \", np.average(f1_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b2fe114f-5f52-422d-957b-a9d6fc3ff31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #classify by RNN\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# X_train = [tf_idf_dict[i[0]] for i in total_train]  # list of abstracts\n",
    "# X_test = [tf_idf_dict[i[0]] for i in total_test] \n",
    "# # Assuming 'document_embeddings' is a list of embeddings and 'labels' is your list of labels\n",
    "# document_embeddings = [tf_idf_dict[i[0]] for i in total_train]\n",
    "\n",
    "# accuracy_reslts = []\n",
    "# precision_results = []\n",
    "# recall_results = []\n",
    "# f1_results = []\n",
    "# for r in tqdm(range(len(total_train[0])-3)):\n",
    "#     # Example training data\n",
    "#     labels = [i[r+3] for i in total_train if i[0] in id_contextual ]  # list of categories based on MeSH terms\n",
    "#     y_test = [i[r+3] for i in total_test if i[0] in id_contextual]  # list of categories based on MeSH terms\n",
    "#     # Encode labels\n",
    "#     label_encoder = LabelEncoder()\n",
    "#     encoded_labels = label_encoder.fit_transform(labels)\n",
    "#     encoded_labels = tf.keras.utils.to_categorical(encoded_labels)\n",
    "\n",
    "#     padded_docs = pad_sequences(document_embeddings, padding='post')\n",
    "#     model = Sequential([\n",
    "#         # Assuming embedding dimension is 100. Adjust 'input_dim' to your vocabulary size,\n",
    "#         # 'output_dim' to the size of your embeddings, and 'input_length' to the length of your padded documents\n",
    "#         Embedding(input_dim=len(X_train[0]), output_dim=2, input_length=padded_docs.shape[1]),\n",
    "#         SimpleRNN(units=64),\n",
    "#         Dense(units=len(np.unique(labels)), activation='softmax')\n",
    "#     ])\n",
    "#     # Compile the model\n",
    "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     # Train the model\n",
    "#     model.fit(padded_docs, encoded_labels, epochs=10, verbose=1)\n",
    "\n",
    "#     new_padded_docs = pad_sequences(X_test, maxlen=padded_docs.shape[1], padding='post')\n",
    "#     # Predict\n",
    "#     predictions = model.predict(new_padded_docs)\n",
    "#     # Decode predicted labels\n",
    "#     predicted_labels = label_encoder.inverse_transform([np.argmax(pred) for pred in predictions])\n",
    "\n",
    "#     accuracy_reslts.append(accuracy_score(y_test, predicted_labels))\n",
    "#     precision_results.append(precision_score(y_test, predicted_labels, zero_division=0, average='weighted'))\n",
    "#     recall_results.append(recall_score(y_test, predicted_labels, average='weighted'))\n",
    "#     f1_results.append(f1_score(y_test, predicted_labels, average='weighted'))\n",
    "\n",
    "# print(\"results for RNN, using tf-idf vectors without difstoryGen\")\n",
    "# print(\"Average  of accuracy: \", np.average(accuracy_reslts))\n",
    "# print(\"Average of percision: \", np.average(precision_results))\n",
    "# print(\"Average of recall: \", np.average(recall_results))\n",
    "# print(\"Average of f1 score: \", np.average(f1_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "a3397b9b-414f-4ccf-9c82-204d5cce6dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1402/1402 [00:00<00:00, 1579.70it/s]\n",
      "100%|██████████| 1402/1402 [00:00<00:00, 64527.05it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "c587ba9d-4be2-4c2c-b25b-0884aefbf10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_75 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_20 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_76 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_21 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25664 (100.25 KB)\n",
      "Trainable params: 25280 (98.75 KB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "88/88 [==============================] - 1s 2ms/step - loss: 4919.6440 - accuracy: 0.0100\n",
      "Epoch 2/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 4855.0718 - accuracy: 0.0128\n",
      "Epoch 3/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 4751.3726 - accuracy: 0.0207\n",
      "Epoch 4/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 4735.8091 - accuracy: 0.0143\n",
      "Epoch 5/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 4695.7046 - accuracy: 0.0136\n",
      "Epoch 6/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 4726.1538 - accuracy: 0.0164\n",
      "Epoch 7/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 4723.7534 - accuracy: 0.0150\n",
      "Epoch 8/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4700.3584 - accuracy: 0.0178\n",
      "Epoch 9/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4696.0068 - accuracy: 0.0228\n",
      "Epoch 10/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4679.7783 - accuracy: 0.0243\n",
      "Epoch 11/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4668.1763 - accuracy: 0.0178\n",
      "Epoch 12/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 4685.5801 - accuracy: 0.0178\n",
      "Epoch 13/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 4666.6899 - accuracy: 0.0207\n",
      "Epoch 14/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 4650.7100 - accuracy: 0.0143\n",
      "Epoch 15/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 4695.9502 - accuracy: 0.0178\n",
      "Epoch 16/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 4671.9536 - accuracy: 0.0121\n",
      "Epoch 17/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 4704.8184 - accuracy: 0.0114\n",
      "Epoch 18/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4684.2188 - accuracy: 0.0100\n",
      "Epoch 19/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4675.3472 - accuracy: 0.0114\n",
      "Epoch 20/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4679.5229 - accuracy: 0.0107\n",
      "Epoch 21/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4656.8804 - accuracy: 0.0136\n",
      "Epoch 22/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4673.1519 - accuracy: 0.0136\n",
      "Epoch 23/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4690.5508 - accuracy: 0.0150\n",
      "Epoch 24/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4666.4585 - accuracy: 0.0136\n",
      "Epoch 25/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4675.5933 - accuracy: 0.0164\n",
      "Epoch 26/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4677.7612 - accuracy: 0.0107\n",
      "Epoch 27/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4667.5522 - accuracy: 0.0086\n",
      "Epoch 28/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4675.0986 - accuracy: 0.0086\n",
      "Epoch 29/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4664.2021 - accuracy: 0.0107\n",
      "Epoch 30/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4680.9019 - accuracy: 0.0100\n",
      "Epoch 31/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4672.9443 - accuracy: 0.0078\n",
      "Epoch 32/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4692.5913 - accuracy: 0.0100\n",
      "Epoch 33/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4701.7188 - accuracy: 0.0150\n",
      "Epoch 34/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4694.2412 - accuracy: 0.0143\n",
      "Epoch 35/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4821.6709 - accuracy: 0.0214\n",
      "Epoch 36/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4687.8384 - accuracy: 0.0214\n",
      "Epoch 37/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4680.4907 - accuracy: 0.0193\n",
      "Epoch 38/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4682.4351 - accuracy: 0.0164\n",
      "Epoch 39/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4694.9312 - accuracy: 0.0164\n",
      "Epoch 40/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4716.8154 - accuracy: 0.0171\n",
      "Epoch 41/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4655.2783 - accuracy: 0.0164\n",
      "Epoch 42/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4639.7686 - accuracy: 0.0136\n",
      "Epoch 43/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4678.9902 - accuracy: 0.0157\n",
      "Epoch 44/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4698.0576 - accuracy: 0.0193\n",
      "Epoch 45/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4690.2388 - accuracy: 0.0193\n",
      "Epoch 46/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4668.8579 - accuracy: 0.0207\n",
      "Epoch 47/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4675.2837 - accuracy: 0.0193\n",
      "Epoch 48/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4696.1675 - accuracy: 0.0221\n",
      "Epoch 49/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4719.7573 - accuracy: 0.0200\n",
      "Epoch 50/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4714.6968 - accuracy: 0.0157\n",
      "Epoch 51/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4697.1289 - accuracy: 0.0143\n",
      "Epoch 52/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4680.4653 - accuracy: 0.0178\n",
      "Epoch 53/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4691.9839 - accuracy: 0.0185\n",
      "Epoch 54/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4655.7983 - accuracy: 0.0164\n",
      "Epoch 55/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4668.0220 - accuracy: 0.0150\n",
      "Epoch 56/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4652.0513 - accuracy: 0.0150\n",
      "Epoch 57/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 4708.7998 - accuracy: 0.0143\n",
      "Epoch 58/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4670.8335 - accuracy: 0.0193\n",
      "Epoch 59/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4667.4370 - accuracy: 0.0136\n",
      "Epoch 60/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4697.7827 - accuracy: 0.0178\n",
      "Epoch 61/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4667.8589 - accuracy: 0.0136\n",
      "Epoch 62/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4680.6079 - accuracy: 0.0136\n",
      "Epoch 63/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4684.9502 - accuracy: 0.0121\n",
      "Epoch 64/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4709.2495 - accuracy: 0.0157\n",
      "Epoch 65/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4661.7188 - accuracy: 0.0143\n",
      "Epoch 66/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4709.5308 - accuracy: 0.0164\n",
      "Epoch 67/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4752.9771 - accuracy: 0.0150\n",
      "Epoch 68/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4688.0537 - accuracy: 0.0100\n",
      "Epoch 69/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4673.8296 - accuracy: 0.0157\n",
      "Epoch 70/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4706.9468 - accuracy: 0.0128\n",
      "Epoch 71/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4651.4688 - accuracy: 0.0107\n",
      "Epoch 72/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4699.1104 - accuracy: 0.0143\n",
      "Epoch 73/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4664.4927 - accuracy: 0.0107\n",
      "Epoch 74/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4635.9546 - accuracy: 0.0143\n",
      "Epoch 75/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4627.1636 - accuracy: 0.0114\n",
      "Epoch 76/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4658.0850 - accuracy: 0.0114\n",
      "Epoch 77/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4661.7178 - accuracy: 0.0093\n",
      "Epoch 78/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4658.7090 - accuracy: 0.0100\n",
      "Epoch 79/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4635.9956 - accuracy: 0.0136\n",
      "Epoch 80/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4632.4175 - accuracy: 0.0100\n",
      "Epoch 81/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4699.1074 - accuracy: 0.0078\n",
      "Epoch 82/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4666.9609 - accuracy: 0.0093\n",
      "Epoch 83/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4637.9136 - accuracy: 0.0114\n",
      "Epoch 84/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4644.9331 - accuracy: 0.0086\n",
      "Epoch 85/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4699.3418 - accuracy: 0.0064\n",
      "Epoch 86/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4660.8755 - accuracy: 0.0064\n",
      "Epoch 87/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4671.2998 - accuracy: 0.0036\n",
      "Epoch 88/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4660.1172 - accuracy: 0.0043\n",
      "Epoch 89/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4676.2573 - accuracy: 0.0029\n",
      "Epoch 90/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4676.2163 - accuracy: 0.0043\n",
      "Epoch 91/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4670.0190 - accuracy: 0.0050\n",
      "Epoch 92/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4672.6411 - accuracy: 0.0029\n",
      "Epoch 93/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4619.7349 - accuracy: 0.0036\n",
      "Epoch 94/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4662.5654 - accuracy: 0.0064\n",
      "Epoch 95/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4655.4346 - accuracy: 0.0057\n",
      "Epoch 96/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4680.0073 - accuracy: 0.0064\n",
      "Epoch 97/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4652.6392 - accuracy: 0.0071\n",
      "Epoch 98/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4680.2598 - accuracy: 0.0100\n",
      "Epoch 99/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 4645.9883 - accuracy: 0.0100\n",
      "Epoch 100/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 4687.2383 - accuracy: 0.0114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f2480c9b5e0>"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "# Define the model\n",
    "def mlp_model(input_dim, output_dim):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,)),  # First hidden layer with 64 neurons and ReLU activation\n",
    "        BatchNormalization(),\n",
    "        Dense(128, activation='relu'),  # Second hidden layer with 32 neurons and ReLU activation\n",
    "        BatchNormalization(),\n",
    "        Dense(64, activation='relu'), \n",
    "        Dense(output_dim, activation='linear')  # Output layer with 'output_dim' neurons\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prepare your input data (X) and labels (y)\n",
    "X = np.array([contextual_dict[i] for i in training_ids])\n",
    "y = np.array([contextual_neighbors[i] for i in training_ids])\n",
    "# Example usage\n",
    "input_dim = X.shape[1] # Size of the input vector (n)\n",
    "output_dim = y.shape[1]  # Size of the output vector (m)\n",
    "\n",
    "# Create the MLP model\n",
    "contextual_model = mlp_model(input_dim, output_dim)\n",
    "\n",
    "# Model summary to see the architecture\n",
    "contextual_model.summary()\n",
    "\n",
    "\n",
    "# X = np.random.rand(number_of_samples, input_dim)\n",
    "# y = np.random.randint(output_dim, size=(number_of_samples, output_dim))\n",
    "# y = tf.keras.utils.to_categorical(y, num_classes=output_dim)\n",
    "\n",
    "# Train the model\n",
    "contextual_model.fit(X, y, epochs=100, batch_size=16)\n",
    "\n",
    "# Note: In this example, 'X' should be a NumPy array or a TensorFlow tensor with shape (number_of_samples, input_dim),\n",
    "# where 'number_of_samples' is the total number of input vectors you have, and 'y' should be the corresponding labels\n",
    "# for each input vector, one-hot encoded to have a shape of (number_of_samples, output_dim).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e46397-74fa-4335-a850-bb6e51880da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "# Define the model\n",
    "def mlp_model(input_dim, output_dim):\n",
    "    model = Sequential([\n",
    "        Dense(int(input_dim/10), activation='relu', input_shape=(input_dim,)),  # First hidden layer with 64 neurons and ReLU activation\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        # Dense(int(input_dim/10), activation='relu'),  # Second hidden layer with 32 neurons and ReLU activation\n",
    "        # BatchNormalization(),\n",
    "        # Dropout(0.5),\n",
    "        # Dense(output_dim, activation='relu'), \n",
    "        Dense(output_dim, activation='linear')  # Output layer with 'output_dim' neurons\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prepare your input data (X) and labels (y)\n",
    "X = np.array([tf_idf_dict[i] for i in training_ids])\n",
    "y = np.array([tf_idf_neighbors[i] for i in training_ids])\n",
    "# Example usage\n",
    "input_dim = X.shape[1] # Size of the input vector (n)\n",
    "output_dim = y.shape[1]  # Size of the output vector (m)\n",
    "\n",
    "# Create the MLP model\n",
    "tf_idf_model = mlp_model(input_dim, output_dim)\n",
    "\n",
    "# Model summary to see the architecture\n",
    "tf_idf_model.summary()\n",
    "\n",
    "\n",
    "# X = np.random.rand(number_of_samples, input_dim)\n",
    "# y = np.random.randint(output_dim, size=(number_of_samples, output_dim))\n",
    "# y = tf.keras.utils.to_categorical(y, num_classes=output_dim)\n",
    "\n",
    "# Train the model\n",
    "tf_idf_model.fit(X, y, epochs=100, batch_size=32)\n",
    "\n",
    "# Note: In this example, 'X' should be a NumPy array or a TensorFlow tensor with shape (number_of_samples, input_dim),\n",
    "# where 'number_of_samples' is the total number of input vectors you have, and 'y' should be the corresponding labels\n",
    "# for each input vector, one-hot encoded to have a shape of (number_of_samples, output_dim).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92efdbe9-a03b-42eb-945e-fbab817990a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_decoder_model = pickle.load(open('../ex_difStoryGen/01_model_tfidf.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff2f69-1a47-4db7-8998-13c924c307f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "4ff374f1-2c22-4227-a19a-886949000a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1402/1402 [00:00<00:00, 1737.19it/s]\n",
      "100%|██████████| 1402/1402 [00:00<00:00, 63002.64it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "0aeda5e3-d129-4c3f-a4ce-ac916e8b7993",
   "metadata": {},
   "outputs": [],
   "source": [
    "convertor_model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "5b9729d6-c926-4355-b928-4eb354bcf0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1402, 64)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "8cfbd7af-52e6-4cce-a801-ac8e296315c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "b5f2587b-7ca4-4e2b-a8cb-0c3e5d78b420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1402, 128)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
